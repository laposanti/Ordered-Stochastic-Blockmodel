---
title: "hierarchical bayes model"
author: "Lapo Santi"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    extra_dependencies: ["algorithm","algpseudocode"]

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(RColorBrewer)
library(truncnorm)
source("/Users/lapo_santi/Desktop/Nial/project/simplified model/Functions_priorSST.R")
```
\newcommand{\vertg}{\rotatebox{90}{$>$}}

# Hierarchical bayesian model

\begin{align}
& K \sim \operatorname{Pois}(\lambda_{K}=1 \mid K>0 ) \quad \text{for } 0< K \leq K^{\max}\\
& a \sim U(0.5,1) \\
& U_1,\ldots, U_K \sim (iid) \quad U(0.5,a) \\
& U_{(k)} \sim f_{U(k)}(u)=\frac{n_0 !}{(k-1) !(n_0-k) !}\left(\frac{u-0.5}{a-0.5}\right)^{k-1}\left(\frac{a-u}{a-0.5}\right)^{n_0-k} \frac{1}{a-0.5} \text {, for } 0.5 \leq u \leq a \text {. }\\
& \sigma^2 \sim f(\sigma^2) = \operatorname{TruncExp}(\lambda_{\sigma^2}=1\mid 0< \sigma^2 < U^{\min}\cdot(1-U^{\min})) , \quad \text{where } U^{\min}= \min \left(\mathbf{U}_{(1:K)}\right)\\
& p_{(k)}\sim f(p_{(k)}\mid \mathbf{U}_{(1:K)}, \sigma^2 ) = Beta(\tilde{\alpha}_k, \tilde{\beta}_k) \quad \text{where } \tilde{\alpha}_k,\tilde{\beta}_k : \begin{cases} 
&\mathbb{E}\left(p_{k}\right) = U_{(k)}\\
&\mathbb{V}ar\left(p_{k}\right) = \sigma^2
\end{cases}\\
& z_i \sim f(z_i \mid \gamma_1,\ldots, \gamma_K) = Mult(n= 1, \gamma_1,\ldots, \gamma_K)\\
&\gamma_i \sim f(\gamma_i \mid A_1,\ldots, A_K ) =  Dir(\gamma_i \mid A_1,\ldots, A_K) \text{ where } A = \sum_{k=1}^K A_k = K \cdot A_k = 1\\
& y_{ij}\sim f(y_{ij} \mid \boldsymbol{z}, p) = Bin\left(n_{ij}, p_{z_i,z_j} \right)
\end{align}

which leads us to:
\begin{align}
& K \sim \operatorname{Pois}(\lambda_{K}=1 \mid K>0 ) \quad \text{for } 0< K \leq K^{\max}\\
& a \sim U(0.5,1) \\
& \mathbf{U}_{(1:K)} \mid a \sim K! \cdot \left( \frac{1}{a - 0.5}\right)^K \\
& \sigma^2 \sim f(\sigma^2) = \operatorname{TruncExp}(\lambda_{\sigma^2}=1\mid \sigma^2 < (U^{\min}\cdot(1-U^{\min}))) , \quad \text{where } U^{\min}= \min \left(\mathbf{U}_{(1:K)}\right)\\
& p_{(1)},\ldots,p_{(K-1)}\mid \mathbf{U}_{(1:K)},\sigma^2 \underset{ind}{\sim} \prod_{k=1}^{K-1} Beta(\tilde{\alpha}_k, \tilde{\beta}_k)\\
& z_{(1)},\ldots,z_{(n)} \mid \gamma_1,\ldots, \gamma_K \underset{ind}{\sim} \frac{\Gamma(\sum_k n_k + 1)}{\prod_k \Gamma(n_k+1)} \prod_{k=1}^K \gamma_k^{n_k} \\ 
&\gamma_1,\ldots, \gamma_{K}\mid \alpha_1,\ldots, \alpha_K  = \frac{1}{\mathrm{B}(\boldsymbol{\alpha})} \prod_{k=1}^K \gamma_k^{\alpha_k - 1}\\
& Y \mid \boldsymbol{z}, P \sim \prod_{p=1}^{K-1} \prod_{q=p+1}^K p_{pq}^{\bar{y}_{pq}} \cdot (1 - p_{pq})^{\bar{n}_{pq}- \bar{y}_{pq}} 
\end{align}

```{r}


# Print the palettes

my_df <- data.frame(level_set = 0, mean_l = 0, perc5 = 0, perc95=0, sd = 0,sample_number=0)
my_combinations = c(0.0001,0.001,0.01,0.1)

for(var_i in 1:length(my_combinations)){
  n_samples=100
  for(sample in 1:n_samples){
    K=5
    a = .80
    U = runif(5,0.5,a)
    x = rgamma(1 ,shape = 1,scale = 1)
    sigma_squared = my_combinations[var_i]
    U_k = sort(U)
    
    beta_params = beta_mean_var(U_k,rep(sigma_squared,K) )
    a_k = beta_params$alpha
    b_k = beta_params$beta
    
    
    
    P_array = array(0,dim=c(K,K,1))
    P = matrix(0,K,K)
    for(k in 0:(K-1)){
      for(i in 1:(K-1)){
        for(j in (i+1):K){
          if((j-i)==k){
            P[i,j]= rbeta(1,a_k[k+1],b_k[k+1])
          }
        }
      }
    }
    
    
    P = P +  lower.tri(P)*(1-t(P))
    diag(P)<- rbeta(K,1,1)
    P_array[,,1]<- P
    
    
    
    gen_samples = generalized_levels(P_array,K = K,N = 1,diag0.5 = F)
    
    
    
    for(e in 1:length(gen_samples)){
      my_df_i<- data.frame(level_set = e, 
                           mean_l = mean(gen_samples[[e]]), 
                           perc5 = quantile(gen_samples[[e]],.05),
                           perc95 = quantile(gen_samples[[e]],.95), 
                           sd = paste('sd=',round(sqrt(sigma_squared),2)),
                           sample_number = sample)
      
      my_df <- rbind(my_df,my_df_i)
    }
  }
}

my_df <- my_df[-1,]




my_df_sum <- my_df %>% group_by(level_set, sd) %>% summarize(mean= mean(mean_l), perc5 = quantile(mean_l,.05), perc95 = quantile(mean_l,.95))


my_df_m<- my_df %>% left_join(my_df_sum, by= c('sd','level_set'))

# Number of colors in the palette
num_colors = length(my_combinations)

# Blue to Red color palette with alpha=0
palette_alpha_0 <- colorRampPalette(c("blue", "red"))(num_colors)

# Blue to Red color palette with alpha=0.5
palette_alpha_0.5 <- adjustcolor(palette_alpha_0, alpha.f = 0.5)

# plotting the level sets
ggplot(my_df_m, aes(x = level_set, y = mean_l, group = sample_number)) +
  facet_grid(~sd) +
  geom_ribbon(aes(ymin = perc5.y, ymax = perc95.y,fill=sd, group=sd)) +
  geom_path(color='gray50', linewidth = 0.4, alpha = 0.05) +
  geom_line(aes(y = mean,color=sd,group=sd),linewidth = .8) +
  scale_color_manual(values = palette_alpha_0) +
  scale_fill_manual(values = palette_alpha_0.5) +
  theme_bw()+
  labs(title = "Evolution of the inter-block probabilities across the level sets",  # Add your desired title
       x = "Level Sets",  # Add your desired X-axis label
       y = expression(p[ij]),  # Add your desired Y-axis label
       fill = "95% confidence interval",
       colour = 'Mean')

```

Here we explore the relationship between $\mathbf{U}_{(1:K)}$ and $\sigma^2$.
Since we know that $\sigma^2 < \mathbf{U}_{(1:K)}(1-\mathbf{U}_{(1:K)})$, this implies that
\[
\frac{1}{2} (1-\sqrt{1-4\sigma^2} <\mathbf{U}_{k} < \frac{1}{2} (1+\sqrt{1-4\sigma^2} \quad \text{ for } k=1,\ldots,K-1
\]

```{r}

#several variances
sigma_squared_combinations= seq(0.0001,0.2,0.001)

#lower bound function
lb_f = function(sigma_squared){
  if(sigma_squared>0){
    lb<- 0.5*(1-sqrt(1-4*sigma_squared))
    return(lb)}
}
#upper bound function
ub_f = function(sigma_squared){
  if(sigma_squared>0){
    ub<- 0.5*(1+sqrt(1-4*sigma_squared))
    return(ub)}
}   

sigma_U_rel_df <- data.frame(
  sigma = sqrt(sigma_squared_combinations),
  sigma_squared = sigma_squared_combinations,
  lb = as.numeric(lapply(sigma_squared_combinations, lb_f)),
  ub = as.numeric(lapply(sigma_squared_combinations, ub_f))
)
#this function is needed to avoid the ggplor error: function non monotonic
my_f <- Vectorize(function(x) {
  if (x < 0) return(x/1e10)
  sqrt(x)
})


ggplot(sigma_U_rel_df, aes(x = sigma_squared, ymin = lb, ymax = ub)) +
  geom_errorbar(color = 'navy') +
  theme_bw() +
  labs(
    title = expression(paste("Admissible ", U[1:K], " values for different ", sigma^2 ,' values')),
    x = expression(sigma^2),
    y = expression(U[1:K])
  ) +
  scale_x_continuous(  limits = c(0,max(sigma_squared_combinations)),
                       sec.axis = sec_axis(~ my_f(.), name =  expression(sigma))
  )




```

# Collapsing P parameter

We have that the likelihood, rewritten over blocks, takes the following expression:
\begin{equation}
f_Y(y \mid z, p)=\prod_{p=1}^K \prod_{q=1}^K \bar{\lambda}_{p q} p_{p q}^{\bar{y}_{p q}} \left(1-p_{p q}\right)^{\bar{m}_{p q}}
\label{eq_likelihood_over_all_blocks}
\end{equation}

where

\begin{align}
&\bar{y}_{p q}=\sum_{i=1}^{N-1} \sum_{j=i+1}^N y_{i j} \quad \mathbb{I}\left(z_i=p\right) \mathbb{I}\left(z_j=q\right)\\
&\bar{m}_{p q}=\sum_{i=1}^{N-1} \sum_{j=i+1}^N\left(n_{i j}-y_{i j}\right) \mathbb{I}\left(z_i=p\right) \mathbb{I}\left(z_j=q\right)\\
&\bar{\lambda}_{p q}=\sum_{i=1}^{N-1} \sum_{j=i+1}^N\left(\begin{array}{l} n_{i j} \\ y_{i j}\end{array}\right) \mathbb{I}\left(z_i=p\right) \mathbb{I}\left(z_j=q\right)
\end{align}

where $\bar{y}_{p q}$ is the number of links between block $p$ and block $q$, $\bar{m}_{p q}$ is the number of missing links between the two blocks, and finally  $\bar{\lambda}_{p q}$ computes the number of ways in which we could have obtained that number of links out of the total amount of possibilities.

The problem with \eqref{eq_likelihood_over_all_blocks} is that the product runs over all possible pairs of blocks. Instead, we would like to have the product spanning just the upper triangular $P$ entries to obtain an expression that would allow us to integrate out the parameter $P$.

We start by rewriting \eqref{eq_likelihood_over_all_blocks} as follows:

\begin{align}
f_Y(y \mid z, p)&=\prod_{p=1}^K \prod_{q=1}^K \bar{\lambda}_{p q} p_{p q}^{\bar{y}_{p q}} \left(1-p_{p q}\right)^{\bar{m}_{p q}}\\
&=\prod_{p=1}^{K} \prod_{q=p}^K \bar{\lambda}_{p q} p_{p q}^{\bar{y}_{p q}} \left(1-p_{p q}\right)^{\bar{m}_{p q}} \cdot \prod_{p=2}^{K} \prod_{q=1}^{K-1} \bar{\lambda}_{p q} p_{p q}^{\bar{m}_{q p}} \left(1-p_{p q}\right)^{\bar{y}_{q p }}\\
&=\prod_{p=1}^{K} \bar{\lambda}_{p p} p_{p p}^{\bar{y}_{p p}} \left(1-p_{p p}\right)^{\bar{m}_{p p}} \cdot \prod_{p=1}^{K-1} \prod_{q=p+1}^K \bar{\lambda}_{p q} \cdot \bar{\lambda}_{qp} p_{p q}^{\bar{y}_{p q} + \bar{m}_{q p}} \left(1-p_{p q}\right)^{\bar{m}_{p q}+ \bar{y}_{q p}}
\end{align}
since $p_{pq} = 1 - p_{qp}$




The prior on the $P$ entries takes the following expression


\begin{equation}
f_p\left(p_{p q} \mid \tilde{\alpha}_k, \tilde{\beta}_k, k\right)= \begin{cases} 
\prod_{p=1}^{K-1} \prod_{q=p+1}^{K} \frac{1}{\operatorname{Beta}\left(\tilde{\alpha}_{k},\tilde{\beta}_{k}\right)} p_{pq}^{\tilde{\alpha}_{k} -1} \left(1-p_{p q}\right)^{\tilde{\beta}_{k}-1}\quad &\text{for } 0<q-p<K-1 \\
\prod_{p=q} Beta(1,1) = [Beta(1,1)]^K\quad &\text{for } q-p=0
\end{cases}
\end{equation}

for $\tilde{\alpha}_k>0, \tilde{\beta}_K>0$, where for $q-p>0$

\begin{align}
& \tilde{\alpha}_k=\left(\frac{1-U_{(k)}}{\sigma^2}-\frac{1}{U_k}\right) \cdot \mu^2 \\
& \tilde{\beta}_k=\tilde{\alpha}_k \cdot\left(\frac{1}{U_{(k)}}-1\right)
\end{align}

so that

\begin{align}
& \mathbb{E}\left(p^{(l)}\right)=U_{(k)} \\
& \operatorname{Var}\left(p^{(l)}\right)=\sigma^2
\end{align}

where $p^{(l)}:=\left\{p_{p q}: q-p=l \quad \text{for } l=1,\ldots,K-1\right\}$, denoted as \textit{Level Set}.

Instead, for $q-p=0$, that is over $p^{(0)}$, we set $\tilde{\alpha}_k=\tilde{\beta}_k=1$, so that the distribution of the level set $p^{(0)}$  , that is, the diagonal of the $P$ matrix is a uniform distribution with

\begin{align}
& \mathbb{E}\left[p^{(0)}\right]=1 / 2 \\
& \mathbb{V}ar\left[p^{(0)}\right]=1 / 12
\end{align}


The, using the beta-binomial conjugacy, we marginalize out $P$, obtaining:

$$
f_Y \left( Y \mid \mathbf{z}, \mathbf{U}_{(1),\ldots,(K)}, \sigma^2, a\right)= \prod_{p=1}^{K-1} \prod_{q=p+1}^{K-1} \left( \bar{\lambda}_{p q}  \bar{\lambda}_{qp}  \frac{B\left(\tilde{\alpha}_k+\bar{y}_{p q}+\bar{m}_{q p}, \tilde{\beta}_k+\bar{m}_{p q}+\bar{y}_{q p}\right)}{B\left(\tilde{\alpha}_k, \tilde{\beta}_k\right)} \right) \prod_{p=1}^K \bar{\lambda}_{p p} \frac{B\left(1 +\bar{y}_{p p}, 1+\bar{m}_{p p}\right)}{B\left(1, 1\right)}
$$





where we use the notation $f_Y \left( y \mid z, \mathbf{U}_{(1),\ldots,(K)}, \sigma^2, a\right)$ to explicit the dependency of $\tilde{\alpha}_k, \tilde{\beta}_k$ upon $\{ \mathbf{U}_{(1),\ldots,(K)}, \sigma^2, \alpha \}$.

To compute it, we use the following function:

```{r}
llike_integrated_out = function(lambdabar, ybar,mbar,K,alpha_k, beta_k){
  llik= matrix(0,K,K)
  for(p in 1:K){
    for(q in p:K){
      if(q-p != 0){
        llik[p,q]<-  log(lambdabar[p,q]) + log(lambdabar[q,p]) +
          lbeta(alpha_k[q-p] + ybar[p,q]+ mbar[q,p],beta_k[q-p] + 
                  ybar[q,p]+ mbar[q,p]) - lbeta(alpha_k[q-p], beta_k[q-p])  
      }else{
        llik[p,p]<-  log(lambdabar[p,q]) +lbeta(1 + ybar[p,q],1 
                                                + ybar[q,p]) - lbeta(1, 1)  
      }
    }
  }
  
  return(sum(llik))
}

```

## Collapsing the prior on $\gamma$

From this model we can write the joint distribution of $( \boldsymbol{z}, \boldsymbol{\boldsymbol{\gamma}})$ as:



\begin{align}
f( \boldsymbol{z}, \boldsymbol{\gamma}) &= f(\boldsymbol{z} | \boldsymbol{\gamma} ) f(\boldsymbol{\gamma} | \boldsymbol{\alpha} ) \nonumber \\
&= \frac{1}{B(\boldsymbol{\alpha})}\gamma^{n_1} \gamma^{n_2} ...\gamma^{n_K}  \prod_{i=1}^K ( \gamma_i^{\alpha_i - 1}) \nonumber  \\
&= \frac{1}{B(\boldsymbol{\alpha})} \prod_{i=1}^K ( \gamma_i^{n_i + \alpha_i - 1})
\end{align}

where $n_k = \sum_{i=1}^N \mathbb{I} \left( z_i = k\right)$ is the number of items in block $k$. 
Now, we can marginalize out $\boldsymbol{\gamma}$ exploiting the Dirichlet-Categorial conjugacy 

\begin{align}
f_{\boldsymbol{z}}(\boldsymbol{z} \mid \boldsymbol{\alpha}) &= \int \mathrm{p}( \boldsymbol{z}, \boldsymbol{\gamma})  \boldsymbol{\gamma} \nonumber \\
&= \frac{\sum_{i=1}^K \Gamma (\alpha_i)}{\prod_{i=1}^K \Gamma(\alpha_i)}  \int \gamma_i^{n_i + \alpha_i - 1} d \gamma \label{eqn:dirichletkernel}\\
&= \frac{\sum_{i=1}^K \Gamma (\alpha_i)}{\prod_{i=1}^K \Gamma(\alpha_i)} \frac{\prod_{i=1}^K \Gamma(\alpha_i + n_i)} {\Gamma (\sum_{i=1}^K \alpha_i + n_i )} \nonumber \\
&= \frac{\Gamma ( \sum_{i =1}^K \alpha_i)}{\Gamma (\sum_{i=1}^K \alpha_i + n_i )} \prod_{i=1}^K  \frac{\Gamma(\alpha_i + n_i)}{\Gamma (\alpha_i)} \nonumber \\
&=  \frac{\Gamma (A)}{\Gamma (A + N)} \prod_{i=1}^K  \frac{\Gamma(\alpha_i + n_i)}{\Gamma (\alpha_i)} \label{eqn: z-marginal}
\end{align}


where we have recognised in \eqref{eqn:dirichletkernel} the kernel of a $\operatorname{Dirichlet}(n_i + \alpha_i)$ and where \eqref{eqn: z-marginal} is known as Dirichlet-Multinomial distribution and we denote it as $DM(\boldsymbol{\alpha})$

# The full proportional posterior distribution

\begin{align}
& f\left(\boldsymbol{z}, \sigma^2, a, \mathbf{U}_{(1:K)} , K \mid Y \boldsymbol{\alpha} \right) \propto \\
& f\left(Y \mid \boldsymbol{z}, \sigma^2, a, \mathbf{U}_{(1:K)}, K\right) \cdot f(\boldsymbol{z} \mid \boldsymbol{\alpha}) \cdot f\left(\mathbf{U}_{(1:K)} \mid a, K\right) \cdot f\left(\sigma^2 \mid U^{\min} \right) \cdot f(a) \cdot f(K) = \\
&\underbrace{\prod_{p=1}^{K-1} \prod_{q=p+1}^{K-1} \left( \bar{\lambda}_{p q}  \bar{\lambda}_{qp}  \frac{B\left(\tilde{\alpha}_k+\bar{y}_{p q}+\bar{m}_{q p}, \tilde{\beta}_k+\bar{m}_{p q}+\bar{y}_{q p}\right)}{B\left(\tilde{\alpha}_k, \tilde{\beta}_k\right)} \right) \prod_{p=1}^K \bar{\lambda}_{p p} \frac{B\left(1 +\bar{y}_{p p}, 1+\bar{m}_{p p}\right)}{B\left(1, 1\right)}}_{f\left(Y \mid \boldsymbol{z}, \sigma^2, a, \mathbf{U}_{(1:K)}, K\right)} \cdot \underbrace{\frac{\Gamma (\boldsymbol{\alpha})}{\Gamma (\boldsymbol{\alpha} + n)} \prod_{i=1}^K  \frac{\Gamma(\alpha_i + n_i)}{\Gamma (\alpha_i)}}_{f(\boldsymbol{z} \mid \boldsymbol{\alpha})} \\
&\cdot \underbrace{K! \cdot \left( \frac{1}{a - 0.5}\right)^K}_{ f\left(\mathbf{U}_{(1:K)} \mid a, K\right)} \cdot \underbrace{\frac{e^{-\sigma^2}}{1-\exp \left(-U^{\min }(1-U^{\min }\right)}}_{f\left(\sigma^2\right)} \underbrace{\frac{1}{2}}_{ f(a)} \cdot \underbrace{\frac{1}{K!(e-1)}}_{f(K)}
\end{align}

Passing to the log:


\begin{equation}
\begin{aligned}
&\log\left(f\left(\boldsymbol{z}, \sigma^2, a, \mathbf{U}_{(1:K)} , K \mid Y \boldsymbol{\alpha} \right) \right) \propto \\ 
& \sum_{p=1}^{K-1} \sum_{q=p+1}^{K-1} \left( \log({\bar{\lambda}_{p q}}) + \log({ \bar{\lambda}_{qp} }) \log({B\left(\tilde{\alpha}_k+\bar{y}_{p q}+\bar{m}_{q p}, \tilde{\beta}_k+\bar{m}_{p q}+\bar{y}_{q p}\right)}) - \log({B\left(\tilde{\alpha}_k, \tilde{\beta}_k\right)}) \right) +\\
&\sum_{p=1}^K( \log(\bar{\lambda}_{p p}) + \log{(B\left(1 +\bar{y}_{p p}, 1+\bar{m}_{p p}\right))} - \log{(B\left(1, 1\right))})+ \log (\Gamma(\boldsymbol{\alpha}))-\log (\Gamma(\boldsymbol{\alpha}+n)) + \\
&\sum_{i=1}^K\left[\log \left( \Gamma\left(\alpha_i+n_i\right)\right) - \log \left(\Gamma\left(\alpha_i\right)\right)\right] +\log (K !)+K \cdot(-\log (a-0.5))  -\sigma^2 -\\
&\log \left(1-\exp \left(-U^{\min }(1-U^{\min })\right)\right)+ \log (1 / 2)-\log (K !)-\log (e-1) \\
\label{eqn_log_proportional_posterior}
\end{aligned}
\end{equation}

Here I write the code to evaluate the function above \eqref{eqn_log_proportional_posterior}.

```{r}

lprop_posterior <- function(lambdabar, ybar,mbar, a, 
                            alpha_vec, n_k,sigma_squared, U_vec,K){
  
  beta_params = beta_mean_var(U_vec,rep(sigma_squared,K) )
  alpha_k = beta_params$alpha
  beta_k = beta_params$beta
  
  
  
  sum1= llike_integrated_out(lambdabar, ybar,mbar,K,alpha_k,beta_k)
  #prior on z
  sum2 <- ddirichlet_multinomial(N,K,n_k, alpha_vec)
  #prior on U
  sum3 <- lfactorial(K)+K*(-log(a-0.5))
  #prior on sigma^2
  sum4 <- -sigma_squared - log(1-exp(min(U*(1-U))))
  #prior on K
  sum5 <- -lfactorial(K) - log(exp(1) -1)
  #pior on a
  sum6 <- 1/2
  result <- sum1 + sum2 + sum3 + sum4 + sum5 + sum6
  return(result)
}

```


# Inference

## Update z

Regarding the $z$ parameter:








## Update $\sigma^2$

Below we report the algorithm employed to update $\sigma^2$


\begin{algorithm}
\begin{algorithmic}
\State $\texttt{Given} \quad \boldsymbol{z}^{(t)}, (\sigma^2)^{(t)}, a^{(t)}, \mathbf{U}_{(1:K)}^{(t)} , K^{(t)}, Y, N, \boldsymbol{\alpha}$
\State
\State \texttt{1. Sample} 
\begin{align} (\sigma^{2})^{\prime}&\sim g \left( (\sigma^{2})^{(t)}, (\tau^2_{\sigma^2})^{(t)}\mid 0, U^{\min}\right) \\
&= \operatorname{TruncNorm}\left(\mu= (\sigma^{2})^{(t)}, \sigma^2 = (\tau^2_{\sigma^2})^{(t)} , lb= 0,ub= U^{\min}(1-U_{\min})\right)\end{align}

\State \texttt{2. Take} $$
(\sigma^{2})^{(t+1)} = 
\begin{cases}
(\sigma^{2})^{(t)} \quad &\texttt{with probability} \quad 1 - r^\prime, \\
(\sigma^{2})^{\prime} \quad &\texttt{with probability} \quad r^\prime,
\end{cases}
$$
\State $\texttt{where}$ \begin{align}\label{eqn:sigma_ratio}
&r^\prime = \log(1) \ \vertg \nonumber \\ 
&\log\left(f\left(\boldsymbol{z}^{(t)}, (\sigma^{2})^{\prime}, a^{(t)}, \mathbf{U}_{(1:K)}^{(t)} , K^{(t)} \mid Y \boldsymbol{\alpha} \right) \right) + \log\left(g((\sigma^{2})^{(t)})\right) - \\
&\log\left(f\left(\boldsymbol{z}^{(t)}, (\sigma^{2})^{(t)}, a^{(t)}, \mathbf{U}_{(1:K)}^{(t)} , K^{(t)} \mid Y \boldsymbol{\alpha} \right) \right)  - \log\left(g((\sigma^{2})^{\prime}) \right) 
\end{align}
\end{algorithmic}
\caption{Metropolis-within-Gibbs update for \( \sigma^2 \)}
\label{alg_general_algorithm_sigma}
\end{algorithm}

Here is the function employed for its computation:


```{r}
#code for the sigma^2 update step within the MH MCMC algorithm

sigma_squared_update = function(lambdabar,ybar,mbar, a, alpha_vec, n_k,
                                sigma_squared, U_vec,K, theta_sigma_squared,
                                acc.count_sigma_squared){
  
  U_max = max(U_vec)
  
  #simulating (sigma^2)' from a g ~ truncated normal
  sigma_squared_prime <- rtruncnorm(1,a = 0,b = 0.9,
                                    mean = sigma_squared,
                                    sd = theta_sigma_squared**2)
  
  #computing the proportional posterior in (sigma^2)'
  prop_posterior_prime <- lprop_posterior(lambdabar, ybar,mbar, a, alpha_vec, 
                                          n_k,sigma_squared_prime, U_vec,K)
  
  #evaluating the proportional posterior in (sigma^2)^(t)
  prop_posterior_current<- lprop_posterior(lambdabar, ybar,
                                           mbar, a, alpha_vec, n_k,
                                           sigma_squared_current, U_vec,K)
  
  #evaluating the proposal density g(sigma^2)') 
  log_proposal_prime <- log(dtruncnorm(sigma_squared_prime,a = 0,b = 0.9,
                                       mean = sigma_squared,
                                       sd = theta_sigma_squared**2))
  
  #evaluating the proposal density g(sigma^2)^(t)) 
  log_proposal_current <- log(dtruncnorm(sigma_squared,a = 0,b = 0.9,
                                         mean = sigma_squared,
                                         sd = theta_sigma_squared**2))
  #acceptance ratio
  log_r=  prop_posterior_prime + log_proposal_current - 
    prop_posterior_current - log_proposal_prime
  
  #create statements that check conditionion to accept move
  MH_condition_S= min(log_r,0)>=log(runif(1))
  if(MH_condition_S){
    acc.count_sigma_squared = acc.count_sigma_squared+1
    sigma_squared <- sigma_squared_prime
  }
  
  
  return(list(acc.moves = acc.count_sigma_squared,
              sigma_squared=sigma_squared))
  
} 

```


## Update a

Below we report the algorithm employed to update $a$


\begin{algorithm}
\begin{algorithmic}
\State $\texttt{Given} \quad \boldsymbol{z}^{(t)}, (\sigma^2)^{(t)}, a^{(t)}, \mathbf{U}_{(1:K)}^{(t)} , K^{(t)}, Y, N, \boldsymbol{\alpha}$
\State
\State \texttt{1. Sample} 
\begin{align} 
a^{\prime} &\sim g \left( a^{(t)}, (\tau^2_{a})^{(t)}\mid 0, 1 \right) \\
&= \operatorname{TruncNorm}\left(\mu= (a^{(t)}, \sigma^2 = (\tau^2_{a})^{(t)} , lb= 0,ub= 1 \right)
\end{align}

\State \texttt{2. Take} $$
a^{(t+1)} = 
\begin{cases}
a^{(t)} \quad &\texttt{with probability} \quad 1 - r^\prime, \\
a^{\prime} \quad &\texttt{with probability} \quad r^\prime,
\end{cases}
$$
\State $\texttt{where}$ \begin{align}\label{eqn:a_ratio}
&r^\prime = \log(1) \ \vertg \nonumber \\ 
&\log\left(f\left(\boldsymbol{z}^{(t)}, (\sigma^{2})^{(t)}, a^{\prime}, \mathbf{U}_{(1:K)}^{(t)} , K^{(t)} \mid Y \boldsymbol{\alpha} \right) \right) + \log\left(g(a^{(t)})\right) - \\
&\log\left(f\left(\boldsymbol{z}^{(t)}, (\sigma^{2})^{(t)}, a^{(t)}, \mathbf{U}_{(1:K)}^{(t)} , K^{(t)} \mid Y \boldsymbol{\alpha} \right) \right)  - \log\left(g(a^{\prime}) \right) 
\end{align}
\end{algorithmic}
\caption{Metropolis-within-Gibbs update for \( a \)}
\label{alg_general_algorithm_a}
\end{algorithm}

Here is the function employed for its computation:


```{r}
#code for the a parameter update step within the MH MCMC algorithm

a_update = function(lambdabar, ybar,mbar, a, alpha_vec, n_k,
                    sigma_squared, U_vec,K, theta_sigma_squared,
                    acc.count_a){
  
  
  
  #simulating (sigma^2)' from a g ~ truncated normal
  a_prime <- rtruncnorm(1,a = 0.5,b = 1,
                        mean = a,
                        sd = theta_a**2)
  
  #computing the proportional posterior in a'
  prop_posterior_prime <- lprop_posterior(lambdabar, ybar,mbar, a_prime, 
                                          alpha_vec, n_k,sigma_squared, 
                                          U_vec,K)
  
  #evaluating the proportional posterior in a^(t)
  prop_posterior_current<- lprop_posterior(lambdabar, ybar,
                                           mbar, a, alpha_vec, n_k,
                                           sigma_squared, U_vec,K)
  
  #evaluating the proposal density g(a') 
  log_proposal_prime <- log(dtruncnorm(a_prime,a = 0,b = 0.9,
                                       mean = sigma_squared,
                                       sd = theta_sigma_squared**2))
  
  #evaluating the proposal density g(sigma^2)^(t)) 
  log_proposal_current <- log(dtruncnorm(1,a = 0,b = 0.9,
                                         mean = a,
                                         sd = theta_a**2))
  #acceptance ratio
  log_r=  prop_posterior_prime + log_proposal_current - 
    prop_posterior_current - log_proposal_prime
  
  #create statements that check conditionion to accept move
  MH_condition_S= min(log_r,0)>=log(runif(1))
  if(MH_condition_S){
    acc.count_a = acc.count_a+1
    a <- a_prime
  }
  
  
  return(list(acc.moves = acc.count_a,
              sigma_squared=sigma_squared))
  
} 

```





## Update U

Below we report the algorithm employed to update $\mathbf{U}_{(1:K)}^{(t)}$


\begin{algorithm}
\begin{algorithmic}
\State $\texttt{Given} \quad \boldsymbol{z}^{(t)}, (\sigma^2)^{(t)}, a^{(t)}, \mathbf{U}_{(1:K)}^{(t)} , K^{(t)}, Y, N, \boldsymbol{\alpha}$
\For{\texttt{k = 1:K}}
\State \texttt{1. Sample}

\begin{align} 
U_{(k)}^{\prime} &\sim g_l= \begin{cases} g_1 \left( U_{(1)}^{(t)}, (\tau^2_{U_{(1)}})^{(t)}\mid 0.5, U_{(2)}^{(t)} \right) &\quad \text{ if } k=1 \\ 
g_2 \left( U_{(k)}^{(t)}, (\tau^2_{U_{(k)}})^{(t)}\mid U_{(k-1)}^{(t)}, U_{(k+1)}^{(t)} \right) &\quad \text{ if } 1<k<K \\ 
g_3 \left( U_{(K)}^{(t)}, (\tau^2_{U_{(K)}})^{(t)}\mid U_{(K-1)},a \right) &\quad \text{ if } k=K \\ 
\end{cases} \\
&= \begin{cases}
\operatorname{TruncNorm}\left(\mu=  U_{(1)}^{(t)}, \sigma^2 = (\tau^2_{1})^{(t)} , lb= 0.5,ub= U_{(2)}^{(t)} \right) &\quad \text{ for } k=1\\
\operatorname{TruncNorm}\left(\mu= U_{(k)}^{(t)}, \sigma^2 = (\tau^2_{k})^{(t)} , lb= U_{(k-1)}^{(t)},ub= U_{(k+1)}^{(t)} \right)&\quad \text{ for }  1<k<K \\
\operatorname{TruncNorm}\left(\mu= U_{(K)}^{(t)}, \sigma^2 = (\tau^2_{K})^{(t)} , lb= U_{(k-1)}^{(t)},ub= U_{(K)}^{(t)} \right)&\quad \text{ for } k=K\\
\end{cases}
\end{align}



\State \texttt{2. Take} $$
U_{(k)}^{(t+1)} = 
\begin{cases}
U_{(k)}^{(t)} \quad &\texttt{with probability} \quad 1 - r^\prime, \\
U_{(k)}^{\prime} \quad &\texttt{with probability} \quad r^\prime,
\end{cases}
$$
\State $\texttt{where}$ 

\begin{align}
&r^\prime = \log(1) \vertg \nonumber \\ 
&\log\left(f\left(\boldsymbol{z}^{(t)}, (\sigma^{2})^{(t)}, a^{(t)}, \mathbf{U}_{(k_j<k)}^{(t+1)}, \mathbf{U}_{(k)}^{\prime}, \mathbf{U}_{(k_j>k)}^{(t)}, K^{(t)} \mid Y \boldsymbol{\alpha} \right) \right) + \log\left(g_l(U_k^{(t)})\right) - \\
&\log\left(f\left(\boldsymbol{z}^{(t)}, (\sigma^{2})^{(t)}, a^{(t)}, \mathbf{U}_{(k_j<k)}^{(t+1)}, \mathbf{U}_{(k:K)}^{(t)}, K^{(t)} \mid Y \boldsymbol{\alpha} \right) \right)  - \log\left(g_l(U_k^{\prime}) \right) 
\label{eqn:U_update}
\end{align}
\EndFor
\end{algorithmic}
\caption{Metropolis-within-Gibbs update for \( \mathbf{U}_{(1:K)}^{(t)} \)}
\label{alg_general_algorithm_a}
\end{algorithm}


Here is the function employed for its computation:


```{r}
#code for the U parameter update step within the MH MCMC algorithm

U_update = function(lambdabar, ybar,mbar, a, alpha_vec, n_k,
                    sigma_squared, U_vec,K, theta_sigma_squared,
                    acc.count_a){
  
  
  for(k in 2:(K+1)){
    
    U_ext <- c(0.5,U_vec,a)
    U_vec_prime <- U_vec
    
    #simulating (sigma^2)' from a g ~ truncated normal
    U_prime <- rtruncnorm(1,a = U_ext[k-1],b = U_ext[k+1],
                          mean = U_vec[k],
                          sd = theta_U[k]**2)
    
    U_vec_prime[k] <- U_prime
    #computing the proportional posterior in a'
    prop_posterior_prime <- lprop_posterior(lambdabar, ybar,mbar, a, 
                                            alpha_vec, n_k,sigma_squared, 
                                            U_vec_prime,K)
    
    #evaluating the proportional posterior in a^(t)
    prop_posterior_current<- lprop_posterior(lambdabar, ybar,
                                             mbar, a, alpha_vec, n_k,
                                             sigma_squared, U_vec,K)
    
    #evaluating the proposal density g(a') 
    log_proposal_prime <- log(dtruncnorm(U_prime,a = U_ext[k-1],b = U_ext[k+1],
                                         mean = U_vec[k],
                                         sd = theta_U[k]**2))
    
    #evaluating the proposal density g(sigma^2)^(t)) 
    log_proposal_current <- log(dtruncnorm(U_vec[k],a = U_ext[k-1],b = U_ext[k+1],
                                           mean = U_vec[k],
                                           sd = theta_U**2))
    #acceptance ratio
    log_r=  prop_posterior_prime + log_proposal_current - 
      prop_posterior_current - log_proposal_prime
    
    #create statements that check conditionion to accept move
    MH_condition_S= min(log_r,0)>=log(runif(1))
    if(MH_condition_S){
      acc.count_a = acc.count_a+1
      U_vec <- U_prime
    }
  }
  
  return(list(acc.moves = acc.count_a,
              U_vec=U_vec))
  
} 

```












## Update K




# Appendix

## Checking that rewriting the likelihood over blocks is correct


First, we are simulating some data and compute the likelihood with the product ranging over individuals $i<j=1,\ldots,n$

```{r}
# Binomial SBM

#Simulating the from the prior P -------
set.seed(12)
n=20
K=4
a = .70
U = runif(4,0.5,a)
x = rgamma(1 ,shape = 1,scale = 1)
sigma_squared = 0.001
U_k = sort(U)

beta_params = beta_mean_var(U_k,rep(sigma_squared,K) )
a_k = beta_params$alpha
b_k = beta_params$beta

N<- matrix(2,n,n)
diag(N)<-0

P = matrix(0,K,K)
for(k in 0:(K-1)){
  for(i in 1:(K-1)){
    for(j in (i+1):K){
      if((j-i)==k){
        P[i,j]= rbeta(1,a_k[k+1],b_k[k+1])
      }
    }
  }
}

P = P +  lower.tri(P)*(1-t(P))
diag(P)=0.5

#simulating z
z = matrix(0,n,1)
z<- sample(1:K, n,replace=T)

z_P<- vec2mat_0_P(z,P)

P_nbyn<- calculate_victory_probabilities(z_P,P)

#simulating N
Y_binom <- matrix(0, n,n)
for(i in 1:n){
  for(j in 1:n){
    Y_binom[i,j]<- rbinom(1,N[i,j],P_nbyn[i,j])
  }
}

Y_binom[lower.tri(Y_binom)] = N[lower.tri(N)] - t(Y_binom)[lower.tri(Y_binom)]
diag(Y_binom)<- 0

#--------------

log_lik_f_binom = function(N,Y,z,P, directed=T){
  z_P<- vec2mat_0_P(z,P)
  P_nbyn<- calculate_victory_probabilities(z_P, P)
  if(directed==T){
    #computing the pairwise log-probabilitiees
    bigM = lchoose(N,Y)+(Y* log(P_nbyn)+(N-Y)*log(1 - P_nbyn))
    #remember to subtract the diagonal
    log_lik= sum(bigM) - sum(diag(bigM))
  }else if(directed==F){
    bigM = lchoose(N,Y)+(Y* log(P_nbyn)+(N-Y)*log(1 - P_nbyn))
    #remember to subtract the diagonal
    log_lik= sum(bigM*upper.tri(bigM))
  }
  return(log_lik)
}

# number of victories between block p and block q
ybar_binom1 = t(z_P)%*%(Y_binom*upper.tri(Y_binom))%*%z_P
# number of missed victories between block p and block q
n_minus_y1 <- (N-Y_binom)*upper.tri(N)
# number of missed victories between block p and block q
mbar_binom1<- t(z_P)%*%n_minus_y1%*%z_P

coef1 = lchoose(N, Y_binom)*upper.tri(N)
bin_coef1 <- t(z_P)%*%(coef1)%*%z_P


llik_over_blocks_f_binomial = function(bin_coef, ybar, mbar, P){
  #llik<- sum(bin_coef+ ybar*log(P) + (mbar)*log(1-P))
  llik = matrix(0,K,K)
  for(p in 1:K){
    for(q in p:K){
      if(q-p==0){
        llik[p,q] = bin_coef[p,q]+ ybar[p,q]*log(P[p,q]) + mbar[p,q]*log(1-P[p,q])
      }else{
        llik[p,q] = bin_coef[q,p]+ bin_coef[p,q]+(ybar[p,q]+mbar[q,p])*log(P[p,q]) + (mbar[p,q]+ybar[q,p])*log(1-P[p,q]) 
      }
    }
  }
  return(sum(llik))
}

llik_over_blocks_f_binomial(bin_coef1 ,ybar_binom1, mbar_binom1,P)

log_lik_f_binom(N,Y_binom,z,P,directed=F)
```


```{r}

llike_integrated_out = function(lambdabar, ybar,mbar,K,alpha_k, beta_k){
  llik= matrix(0,K,K)
  for(p in 1:K){
    for(q in p:K){
      if(q-p != 0){
        llik[p,q]<-  log(lambdabar[p,q]) + log(lambdabar[q,p]) +
          lbeta(alpha_k[q-p] + ybar[p,q]+ mbar[q,p],beta_k[q-p] + 
                  ybar[q,p]+ mbar[q,p]) - lbeta(alpha_k[q-p], beta_k[q-p])  
      }else{
        llik[p,p]<-  log(lambdabar[p,q]) +lbeta(1 + ybar[p,q],1 
                                                + ybar[q,p]) - lbeta(1, 1)  
      }
    }
  }
  
  return(sum(llik))
}

```

## Checking that the full likelihood works effectively

```{r}

P_prior_probability = function(P,K,U_vec, sigma_squared){
  
  beta_params = beta_mean_var(U_vec,rep(sigma_squared,K) )
  alpha_k = beta_params$alpha
  beta_k = beta_params$beta
  
  p_P = matrix(0,K,K)
  for(p in 1:K){
    for(q in p:K){
      if(q-p != 0){
        p_P[p,q]<-  dbeta(P[p,q],alpha_k[q-p],beta_k[q-p],log = TRUE) 
      }else if (p==q){
        p_P[p,p]<-  dbeta(P[p,q],1,1,log = TRUE) 
      }
    }
  }
  
  return(sum(p_P))
}

```


```{r}
N_samples = 1000


set.seed(12)
n=100
N<- matrix(2,n,n)
diag(N)<-0
K= 5
a = .75
sigma_squared = 0.005
P_container = array(0, dim=c(K,K,N_samples))

Y_container= array(0, dim=c(n,n,N_samples))
z = matrix(0,n,1)
z<- sample(1:K, n,replace=T)
U = runif(K,0.5,a)
U_k = sort(U)


for(sample in 1:N_samples){
  
  beta_params = beta_mean_var(U_k,rep(sigma_squared,K) )
  
  a_k = beta_params$alpha
  b_k = beta_params$beta
  
  
  P = matrix(0,K,K)
  for(k in 0:(K-1)){
    for(i in 1:(K-1)){
      for(j in (i+1):K){
        if((j-i)==k){
          P[i,j]= rbeta(1,a_k[k+1],b_k[k+1])
        }
      }
    }
  }
  
  P = P +  lower.tri(P)*(1-t(P))
  diag(P)= rbeta(K,1,1)
  P_container[,,sample]<-P
  
  z_P<- vec2mat_0_P(z,P)
  
  P_nbyn<- calculate_victory_probabilities(z_P,P)
  
  #simulating N
  Y_binom <- matrix(0, n,n)
  for(i in 1:n){
    for(j in 1:n){
      Y_binom[i,j]<- rbinom(1,N[i,j],P_nbyn[i,j])
    }
  }
  
  Y_binom[lower.tri(Y_binom)] = N[lower.tri(N)] - t(Y_binom)[lower.tri(Y_binom)]
  diag(Y_binom)<- 0
  
  Y_container[,,sample]<- Y_binom
}
```


```{r}

sigma_squared_values <-  seq(0.000001,.1,.001)

prior_P = matrix(0,length(sigma_squared_values),1)

for(try_out in 1:length(sigma_squared_values)){
  prior_container = matrix(0,N_samples,1)
  
  beta_params = beta_mean_var(U_k,rep(sigma_squared_values[try_out],K) )
  
  
  for(try_in in 1:N_samples){
    
    alpha_k = beta_params$alpha
    beta_k = beta_params$beta
    
    Y_binom <- Y_container[,,try_in]
    
    P = matrix(0,K,K)
    for(k in 0:(K-1)){
      for(i in 1:(K-1)){
        for(j in (i+1):K){
          if((j-i)==k){
            P[i,j]= rbeta(1,alpha_k[k+1],beta_k[k+1])
          }
        }
      }
    }
    
    P = P +  lower.tri(P)*(1-t(P))
    diag(P)= rbeta(K,1,1)
    # number of victories between block p and block q
    ybar = t(z_P)%*%(Y_binom*upper.tri(Y_binom))%*%z_P
    # number of missed victories between block p and block q
    n_minus_y1 <- (N-Y_binom)*upper.tri(N)
    # number of missed victories between block p and block q
    mbar<- t(z_P)%*%n_minus_y1%*%z_P
    
    coef1 = lchoose(N, Y_binom)*upper.tri(N)
    lambdabar <- t(z_P)%*%(coef1)%*%z_P
    
    prior_container[try_in]<- llik_over_blocks_f_binomial(bin_coef = lambdabar,ybar = ybar,mbar = mbar,P = P)
  }
  prior_P[try_out] <- sum(prior_container)
  print(sum(prior_container))
}

a_df = data.frame(sigma_squared_values = sigma_squared_values,prior_probabilities = prior_P)

ggplot(a_df, aes(x = sigma_squared_values, y = prior_P))+
  geom_point()+
  geom_vline(xintercept = sigma_squared)+
  geom_vline(xintercept = sigma_squared_values[which.max(prior_P)])


```







