\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Beta reparametrization}
\author{LS}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}
\tableofcontents
\section{Current Model: focus on likelihood and prior for $P$}
Our current likelihood is:
\begin{align}
p(y| z, P, K) &= \prod_{i =1 }^{N-1} \prod_{j =i+1}^{N} p(y_{ij} | z, P, K) \\ 
&= \prod_{i =1 }^{N-1} \prod_{j =i+1}^{N}  {n_{ij} \choose y_{ij}} p_{z_i, z_j}^{y_{ij}}(1- p_{z_i, z_j})^{n_{ij}-y_{ij}}
\end{align}

The prior probability distribution on the $P$ matrix entries $p_{pq}$ is:

\begin{align}
&p \left(p_{pq} \mid \mu_{k}, \sigma^2, \beta_{max} \right) = \\
&\begin{cases}
\operatorname{TruncNorm}\left(\mu_{k}, \sigma^2\right) \mathbb{I}\left(0.5,\beta_{\max}\right) \quad  &\text{for } k= q-p >0\\
0.5 \quad \text{with } p=1 &\text{for } k= q-p =0\\
1-p_{pq} \quad \text{with } p=1 &\text{for } k= q-p <0\\
\end{cases}
\end{align}

where $k$ is the index of the level set $L_{(k)} \subset P$, the collection of the entries satisfying the condition \(p - q = k\). In other words, they correspond to the entries on the \(k\)-th diagonal above the main diagonal within matrix \(P\). Additionally, we set the probabilities within the main diagonal to be 0.5, reflecting an equal chance of preference if two items belong to the same block. The maximum attainable probability is determined by the user and denoted as \(\beta_{\max}\), where \(\beta_{\max}<1\) . This constraint is imposed to prevent extreme probabilities, such as values near 0 or 1.

To enforce the constraint $0.5 < \mu_{(1)} < \mu_{(2)} < \ldots < \mu_{(K-1)}< \beta_{max}$, we consider a power-law function $\mu_{\alpha,k} = 0.5 + g(k)^\alpha \mathbb{I}\left(0.5,\beta_{\max}\right)$. This power law is monotonically increasing in $g(k)$, and it is flexible enough to account for different increasing rates by modulating accordingly the $\alpha$ values. Indeed, we introduce the $\alpha$ hyperparameter, upon which, jointly with $k$, $\mu_{\alpha,k}$ depends as follows:

\begin{align}
\mu_{\alpha,k} &= f(\alpha, k) = \left( \frac{(\beta_{\max} - 0.5)^{(1/\alpha)}}{K} \times k \right)^\alpha + 0.5 \quad \text{for } k = 0, \ldots, K
\end{align}

The equation above may look rather abstract, but the interpretation for $\alpha$ is quite straightforward. The parameter $\alpha$ regulates the rate of increase of the means $\mu_{\alpha,k}$ across the level sets, from $k=1$ to $k=K$. We have an interpretation for \(\alpha\) magnitude:
\begin{itemize}
    \item For \(\alpha\) values exceeding 1, a convex power-law function emerges, engendering a steady but accelerating increase in the level set truncations toward \(\beta_{\max}\).
    
    \item When \(\alpha\) is between 0 and 1, the power-law function becomes concave, promptly pushing values toward \(\beta_{\max}\). This reflects a pronounced bias toward higher probabilities.
    
    \item Notably, as \(\alpha\) is equal to 1, the power-law function becomes linear, leading to a constant increment in the $\mu_{\alpha,k}$ parameter.
\end{itemize}

\begin{figure}
\begin{center}
\includegraphics[width=.60\textwidth,natwidth=576,natheight=482]{/Users/lapo_santi/Desktop/Nial/grafici/prior_exploration/level_sets_means.png}
\caption{ }
\label{ }
\end{center}
\end{figure}

\vspace{.5cm}






\section{Re-parametrise with a Beta and integrate out P}
We first redefine the proportional likelihood over the blocks, meaning that instead of considering the victories/games between players, we count the victories/games occurring between blocks. 
Therefore, we introduce two new quantities:
\begin{align}
\bar{y}_{pq} &= \sum_{i=1}^{N-1} \sum_{j=i+1}^{N} y_{ij} \cdot  \mathbb{I}\left(z_i = p, z_j =q \right)\\
\bar{n}_{pq} &= \sum_{i=1}^{N-1} \sum_{j=i+1}^{N} n_{ij} \cdot  \mathbb{I}\left(z_i = p, z_j =q \right)
\end{align}
where 
\[
\mathbb{I}\left(z_i = p, z_j =q \right) =
\begin{cases}
1 &\text{ if }z_i = p, z_j =q\\
0 &\text{ otherwise}\\
\end{cases}
\]

The likelihood expressed over the blocks will look like:
\[
p(y \mid z,P) \propto \prod_{p=1}^{K-1} \prod_{q=p+1}^K p_{pq}^{\bar{y}_{pq}} \cdot (1 - p_{pq})^{\bar{n}_{pq}- \bar{y}_{pq}} 
\]
where $\bar{n}_{pq}$ and $\bar{y}_{pq}$ are respectively the number of games played between block $p$ and block $q$ and the number of victories of block $p$ vs block $q$. 

We can replace the truncated normals over the level sets with beta distribution so that $p_{pq} \sim \operatorname{TrunBeta}\left(\alpha_k, \beta_k \right)\mathbb{I}_{p_{pq}}\left(0.5,\beta_{max}\right)$. 
We will have the product of $K-1$ truncated beta priors.

\begin{align}
p(P \mid z) &\propto \prod_{k=1}^{K-1} \left( p_{pq}^{\alpha_k-1} \cdot (1 - p_{pq})^{\beta_k -1} \right)^{K-k} \mathbb{I}\left(p_{pq} \in \left[0.5,\beta_{max}\right]\right)\\
&= \prod_{p=1}^{K-1}\prod_{q= p+1}^{K} \left( p_{pq}^{\alpha_k-1} \cdot (1 - p_{pq})^{\beta_k -1} \right) \mathbb{I}\left(p_{pq} \in \left[0.5,\beta_{max}\right]\right)
\end{align}

To induce the same constraint as above, we could reparametrise the Beta by expressing $\alpha_k$ and $\beta_k$ parameters in terms of the mean $\mu_{\alpha,k}$ and the confidence in the mean prior $\nu$. 

We have that:
\begin{align}
\mathbb{E}\left[p_{pq}\mid z\right]&= \frac{\alpha_k}{\alpha_k + \beta_k} \cdot \underbrace{\frac{1}{F(\beta_{max};\alpha_k, \beta_k) - F(0.5;\alpha_k, \beta_k)}}_{Z_0}\nonumber \\ 
&\approx \frac{\alpha_k}{\alpha_k + \beta_k} \quad \text{for } \sigma^2 <0.001
\end{align}

\subsection{Riparametrization with $\nu$ as prior sample size}
In terms of estimating $p_{pq}$, $\mu_{\alpha,k}$ represents our prior guess at the true value of $p_{pq}$ and $\nu = \alpha_k + \beta_k$ represents our confidence in this guess, expressed on the same scale as the sample size. Therefore we can approximate their prior beliefs about $p_{pq}$ with a truncated beta distribution having parameters 
\begin{align}
\alpha_k& =\mu_{\alpha,k}\cdot Z_0 \cdot \nu \nonumber \\
\beta_k &= \nu\cdot \left(1-\mu_{\alpha,k}\cdot Z_0\right)
\end{align}
where $Z_0$ is the normalising constant of the prior truncated distribution that we express in \eqref{eq_normalising_constant}, and $\alpha_k,
\beta_k$ have been both obtained by solving the system of equations $\mathbb{E}\left(p_{pq} \right) = \mu_{\alpha,k}$ and $\alpha_k+\beta_k = \nu$ and $p_{pq}\sim \operatorname{TruncBeta}\left(\alpha_k, \beta_k\right)$.

\subsection{Riparametrisation with mean and variance}

If we were working with a non-truncated beta we may reparametrise the $\operatorname{Beta}\left(\alpha_k,\beta_k\right)$
\begin{align}
\alpha_k&=\mu_{\alpha,k} \left(\frac{\mu_{\alpha,k}(1-\mu_{\alpha,k})}{\sigma^2}-1\right), \text{ if } \sigma^2< \mu_{\alpha,k}(1-\mu_{\alpha,k})\\
\beta_k &= (1 - \mu) \nu = (1 - \mu_{\alpha,k})\left(\frac{\mu_{\alpha,k}(1-\mu_{\alpha,k})}{\sigma^2}-1\right), \text{ if }\sigma^2< \mu_{\alpha,k}(1-\mu_{\alpha,k}).
\end{align}

However, we are working with a truncated beta, and the parameters correspondence is not that straightforward.


\subsection{Integrating $P$ out}

The first step is to show that the beta-binomial conjugacy is preserved under truncation of the prior.

\begin{align}
&p \left(P \mid y, z \right) \propto  p\left(y \mid z, P\right) \cdot p(P \mid z) \\
& \propto \prod_{p=1}^{K-1} \prod_{q=p+1}^K p_{pq}^{\bar{y}_{pq}} \cdot (1 - p_{pq})^{\bar{n}_{pq}- \bar{y}_{pq}} 
\cdot \prod_{p=1}^{K-1}\prod_{q= p+1}^{K} \left( p_{pq}^{\alpha_k-1} \cdot (1 - p_{pq})^{\beta_k -1} \right) \mathbb{I}\left(p_{pq} \in \left[0.5,\beta_{max}\right] \right) \\
& =  \prod_{p=1}^{K-1}\prod_{q= p+1}^{K} p_{pq}^{\bar{y}_{pq}+\alpha_k-1} \cdot\left(1-p_{pq}\right)^{\bar{n}_{pq}-\bar{y}_{pq}+\beta_k -1} \mathbb{I}\left(p_{pq} \in \left[0.5,\beta_{max}\right] \right)\label{eq_integratingPout_integral}
\end{align}

which is the kernel of a truncated beta distributions $\operatorname{TruncBeta}\left(\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}\right)\mathbb{I}\left(p_{pq} \in \left[0.5,\beta_{\max}\right] \right)$, where $\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}$ are:
\begin{align}
\bar{\alpha}_{pq,k} &= \bar{y}_{pq}+\alpha_k \label{eq_alpha_k} \\
\bar{\beta}_{pq,k} &= \bar{n}_{pq}-\bar{y}_{pq}+\beta_k \label{eq_beta_k}
\end{align}

and the normalising constant is
\begin{equation}
F(\beta_{max};\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}) - F(0.5;\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}) 
\label{eq_normalising_constant}
\end{equation}
    
where $F(x;\alpha,\beta)$ is the cumulative distribution function of a $\operatorname{Beta}\left(\alpha,\beta\right)$ distribution evaluated in $x$.

The expected value of such a distribution is:

\begin{align}
\mathbb{E}\left(p_{pq}\mid y,z\right) &= \frac{\bar{\alpha}_{pq,k}}{\bar{\alpha}_{pq,k}+\bar{\beta}_{pq,k}} \cdot \frac{1}{F(\beta_{max};\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}) - F(0.5;\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k})} \nonumber \\
&= \frac{\bar{\alpha}_{pq,k}}{\bar{\alpha}_{pq,k}+\bar{\beta}_{pq,k}} \cdot \frac{1}{Z} = \frac{\bar{y}_{pq}+\alpha_k}{\bar{n}_{pq}+ \alpha_k +\beta_k} \frac{1}{Z} \nonumber \\
&= \left[ \frac{\bar{n}_{pq}}{\bar{n}_{pq}+ \alpha_k +\beta_k} \cdot \frac{\bar{y}_{pq}}{\bar{n}_{pq}} + \frac{\alpha_k +\beta_k}{\bar{n}_{pq}+ \alpha_k +\beta_k} \cdot \frac{\alpha_k}{\alpha_k +\beta_k} \right ] \frac{1}{Z} \nonumber \\
&= \frac{\bar{n}_{pq}}{\bar{n}_{pq}+ \alpha_k +\beta_k} \cdot \underbrace{\frac{\bar{y}_{pq}}{\bar{n}_{pq}} \frac{1}{Z}}_\text{rescaled sample mean}
+ \frac{\alpha_k +\beta_k}{\bar{n}_{pq}+ \alpha_k +\beta_k} \cdot \underbrace{\frac{\alpha_k}{\alpha_k +\beta_k}\frac{1}{Z}}_\text{rescaled prior mean}
 \end{align}

Now, we marginalise out the $P$ parameter, by integrating it out from the joint distribution as follows:

\begin{align}
p \left(y \mid z \right)&= \int_0^1 p\left(y, P \mid z\right) d P =\int_0^1 p \left(y \mid z, P\right) \cdot p(P \mid z) d P \\
& \propto \int_{0.5}^{\beta_{\max}} \prod_{p=1}^{K-1}\prod_{q= p+1}^{K} p_{pq}^{\bar{y}_{pq}+\alpha_k-1} \cdot\left(1-p_{pq}\right)^{\bar{n}_{pq}-\bar{y}_{pq}+\beta_k -1} dp_{pq}
 \label{eq_integratingPout_integral}
\end{align}

In the integral above, we recognise the product of $K \cdot \left( K -1 \right)$ truncated beta distributions $\operatorname{TruncBeta}\left(\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}\right)\mathbb{I}\left(p_{pq} \in \left[0.5,\beta_{\max}\right] \right)$, where $\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}$ are as in \eqref{eq_alpha_k} and \eqref{eq_beta_k}.

Employing a well-known simplification strategy, we divide the expression within the integral by the truncated beta's normalising constant already mentioned in \eqref{eq_normalising_constant}

Now, we multiply and divide \eqref{eq_integratingPout_integral} by the normalising constant:

\begin{align}
& \prod_{p=1}^{K-1}\prod_{q= p+1}^{K}  F(\beta_{max};\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}) - F(0.5;\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k})  \cdot 
   \nonumber \\ 
 &\int_{0.5}^{\beta_{\max}} \prod_{p=1}^{K-1}\prod_{q= p+1}^{K} \frac{1}{F(\beta_{\max};\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}) - F(0.5;\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}) }
 p_{pq}^{\bar{\alpha}_{pq,k}} \cdot\left(1-p_{pq}\right)^{\bar{\beta}_{pq,k}} 
\end{align}

The expression within the integral now is just a product of density, and therefore it equals one. We are left with:
\begin{align}
p \left(y \mid z \right) &\propto \prod_{p=1}^{K-1}\prod_{q= p+1}^{K}  F(\beta_{max};\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}) - F(0.5;\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}) 
\end{align}
where $\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}$ are as in \eqref{eq_alpha_k} and \eqref{eq_beta_k}.

Also, to estimate $P$ is possible, since $p\left(p_{pq} \mid y,z\right) \sim \operatorname{TruncBeta}\left(\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}\right) \mathbb{I}\left(p_{pq} \in \left[0.5,\beta_{\max}\right]\right)$ and therefore 

\begin{align}
\mathbb{E}\left(p_{pq}\right) &= \frac{\bar{\alpha}_{pq,k}}{\bar{\alpha}_{pq,k}+\bar{\beta}_{pq,k}} \cdot \frac{1}{F(\beta_{max};\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k}) - F(0.5;\bar{\alpha}_{pq,k},\bar{\beta}_{pq,k})} \nonumber \\
&= \frac{\bar{\alpha}_{pq,k}}{\bar{\alpha}_{pq,k}+\bar{\beta}_{pq,k}} \cdot \frac{1}{Z} = \frac{\bar{y}_{pq}+\alpha_k}{\bar{n}_{pq}+ \alpha_k +\beta_k} \frac{1}{Z} \nonumber \\
&= \left[ \frac{\bar{n}_{pq}}{\bar{n}_{pq}+ \alpha_k +\beta_k} \cdot \frac{\bar{y}_{pq}}{\bar{n}_{pq}} + \frac{\alpha_k +\beta_k}{\bar{n}_{pq}+ \alpha_k +\beta_k} \cdot \frac{\alpha_k}{\alpha_k +\beta_k} \right ] \frac{1}{Z} \nonumber \\
&= \frac{\bar{n}_{pq}}{\bar{n}_{pq}+ \alpha_k +\beta_k} \cdot \underbrace{\frac{\bar{y}_{pq}}{\bar{n}_{pq}} \frac{1}{Z}}_\text{rescaled sample mean}
+ \frac{\alpha_k +\beta_k}{\bar{n}_{pq}+ \alpha_k +\beta_k} \cdot \underbrace{\frac{\alpha_k}{\alpha_k +\beta_k}\frac{1}{Z}}_\text{rescaled prior mean}
 \end{align}




\section{Allowing for different values on the diagonal}

Here we want to relax the assumption that the entries on the diagonal are fixed to 0.5, since this might be too restricting and could significantly lower the goodness of fit of the model. Instead, we model the level set 0, that is, the main diagonal, as a truncated normal centred around 0.5. The truncation spans the whole restricted probability space from $1-\beta_{\max}$ to $\beta_{\max}$.
The prior probability distribution on the $P$ matrix entries $p_{pq}$ is:

\begin{align}
&p \left(p_{pq} \mid \mu_{k}, \sigma^2, \beta_{\max} \right) = \\
&\begin{cases}
\operatorname{TruncNorm}\left(\mu_{k}, \sigma^2\right) \mathbb{I}\left(p_{pq} \in \left[0.5,\beta_{max}\right]\right) \quad  &\text{for } k= q-p >0\\
\operatorname{TruncNorm}\left(0.5, \sigma^2\right) \mathbb{I}\left(1 - \beta_{\max},\beta_{\max}\right) &\text{for } k= q-p =0\\
1-p_{pq} \quad \text{with } p=1 &\text{for } k= q-p <0\\
\end{cases}
\end{align}


$$\left[\text{to be continued}\right]$$




\end{document}  