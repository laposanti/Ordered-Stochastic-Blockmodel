\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{lscape}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{longtable}
\usepackage{epstopdf}
\usepackage{verbatim}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{subfigure}

\usepackage{algorithm}
\usepackage{algpseudocode}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newcommand{\vertgeq}{\rotatebox{90}{$\leq$}}
\newcommand{\vertg}{\rotatebox{90}{$>$}}
\title{Draft}
\author{Lapo Santi}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}
\tableofcontents

\newpage

\section{Introduction}



When faced with a multitude of alternatives, individuals often strive to organize them into coherent blocks or groups to better understand the decision landscape. Furthermore, they aim to establish a meaningful order within these blocks, enabling them to prioritize alternatives based on preference. This process involves two fundamental tasks: block clustering and order-based ranking. While clustering involves categorizing alternatives into distinct blocks, ranking focuses on arranging these blocks in a specific order. These tasks are typically accomplished through human judgments, often in the form of pairwise comparisons.

In block clustering, the aim is to determine the inherent similarities among alternatives and group them accordingly. By comparing pairs of alternatives, individuals can identify common characteristics, shared attributes, or comparable features that contribute to their clustering. This process helps unveil the underlying structure of the alternatives, allowing decision-makers to comprehend the relationships and associations between them. Several techniques, such as hierarchical clustering and k-means clustering, have been employed to address this task effectively.

Conversely, in order-based ranking, the primary objective is to establish a preference-based order among the identified blocks of alternatives. By comparing pairs of blocks, individuals can discern the relative favorability of one block over another. These pairwise comparisons generate a ranking list that encapsulates the perceived preference or priority of each block. Various methodologies, including the Bradley-Terry model and pairwise comparison matrices, have been utilized to derive meaningful rankings from the collected preferences.

In this article, we propose a novel approach, termed Block Clustering and Order-based Ranking (BCOR), which unifies the tasks of block clustering and order-based ranking into a cohesive framework. The BCOR model introduces a dynamic parameter that governs the granularity of block clustering, allowing decision-makers to explore a spectrum of clustering options. By iteratively adjusting this parameter, the model can encompass a wide range of decision-making scenarios, from finely differentiated blocks to coarser groupings.

A key insight of the BCOR model lies in its ability to relate the number of blocks to the underlying ranking structure. As the number of blocks converges to the total number of alternatives, the model effectively transitions into a traditional ranking approach, providing a complete ordering of the alternatives. Conversely, by intentionally reducing the number of blocks, decision-makers are presented with distinct groups of choices, each requiring preference considerations within its own subset. This approach offers a nuanced perspective on decision-making, allowing individuals to differentiate between highly favored groups and those that are comparatively less preferred.

The BCOR model provides a flexible and adaptive solution for organizing and prioritizing alternatives in various decision-making contexts. Its application extends beyond conventional clustering and ranking tasks, empowering decision-makers to explore the continuum between comprehensive rankings and granular groupings. Additionally, the model can be tailored to incorporate different types of pairwise comparisons, enabling its utilization in diverse domains and decision scenarios.

To evaluate the effectiveness of the BCOR model, we conducted experiments using real-world datasets encompassing a wide range of decision contexts. The results demonstrate the model's ability to generate meaningful block clusters and order-based rankings, outperforming traditional approaches that solely focus on clustering or ranking tasks.

The contributions of this work can be summarized as follows:

We introduce the novel problem of block clustering and order-based ranking, bridging the gap between these two fundamental decision tasks.
We propose the BCOR model, which provides a unified framework to accommodate various levels of granularity in decision-making, from complete rankings to distinct preference-based groups.
We showcase the versatility of the BCOR model through experiments on real-world datasets, highlighting its superior performance compared to existing methods.
By integrating block clustering and order-based ranking, the BCOR model offers decision-makers a comprehensive tool to navigate complex decision landscapes






\newpage
\section{Posets and Dags}
\begin{definition}\label{def:Poset}[Poset]To define poset, a partially ordered set we start from $D$, a set of elements. The binary relation $\prec$ on $D$ is said to be a partial order if:
\begin{align}
&\text{For any } x \in D, x \prec x (\text{ reflexivity }) \\
&\text{For any } x,y,z \in D, x \prec y \text{ and } y \prec z \implies x \prec z (\text{ transitivity }) \\
&\text{For any } x, y \in D, x \prec y \text{ and } y \prec x \implies x = y (\text{ antisymmetry }).
\end{align}
Then we call $(D, \prec)$ a partially ordered set, or a \textit{poset}. 
\end{definition}

A \textit{finite poset} $(D,\prec)$ is a poset where $D$ has a finite number of distinct elements. 

Example: let $D$ be the finite set defined by representing the $M \times N$ array of probabilities. Let $(u,v)$ and (q,r) be any two elements of $D$ and define the binary relation on $D$ by
\begin{equation}
(q,r) \prec (u,v) \iff q \prec u \text{ and } r \prec v
\end{equation}

There exists a correspondence between posets and directed cyclic graphs. 

Let $(D, F)$ be a directed acyclic graph, where $D = \{y_1, \ldots, y_n\}$, a finite set. To construct a poset to which this digraph corresponds, we define the binary relation $\prec$ on D by
\begin{align}
&y_i \prec y_i \text{ for } i= 1\dots n \\ 
&y_i \prec y_j \text{ if there exists a directed path from } y_i \text{ to } y_j \in (D,F)
\end{align}

We saw above that the correspondence is many-to-one. Given a finite poset, one may construct a class of directed acyclic graphs; the correspondence described above is in a sense the minimal directed acyclic graph since it has the smallest possible directed edge set
Pomms definitions 


\begin{definition}[Cone]
For any $y \in D$, the \textit{cone} of $y$ is the set
\begin{equation}
\text{ cone } y = \{x \in D: x\prec y;x\neq y \} \nonumber
\end{equation}
\end{definition}

\begin{definition}[Adjl]
For any $y \in D$ the \textit{adjacent lower neighbourhood} of $y$ is the set $$
\text{adjl } y = \{ x \in D:(x,y) \text{is a directed edge in} (D,F)\}
$$
\end{definition}

\begin{definition}[Dilation]
For any $y \in D$, the \textit{dilation} of $y$ is the set 
\begin{equation}
\text{dil } y = \bigcup\{ \overline{\text{adjl }} x : y \in \overline{\text{adjl }} \} \nonumber
\end{equation}
\end{definition}

\begin{definition}[Excluded dilation]
For any $y \in D$, the \textit{excluded dilation}  of $y$ is the set 
\begin{equation}
\text{dil }^* y = \text{dil } y \setminus \{y\}
\end{equation}
\end{definition}

\begin{definition}[Minimal element]
In general, an element $y\in D$ is called \textit{minimal element} if there is no other element $x$ satisfying $x\prec y$ where $\text{ adjl } s$ is the set of adjacent lower neighbors of $s \in D$.
\end{definition}


\begin{definition}[Cover of a Subset]
The cover of a subset $B$ is a set of all elements $x$ in $D$ such that $x$ is adjacent to an element in $B$ and $x$ is not in $B$. Formally, the cover of $B$ is defined as follows:
\begin{equation}
\text{covr}\ B = { x \in D: \text{adjl}\ x \subset B \text{ and } x \notin B } \nonumber
\end{equation}
where $\text{adjl}\ x$ is the set of all adjacent elements of $x$ in $D$.
\end{definition}
Intuitively, the cover of a subset $B$ represents all the elements in $D$ that are outside of $B$ but are adjacent to at least one element in $B$. In other words, the cover of $B$ captures the neighborhood of $B$ in $D$.

\begin{definition}[Level Sets]
The level sets of a poset $D$ are a sequence of nonempty cover sets defined recursively as follows:
\begin{equation}
L^0 = D_{min};\quad L^i = \text{covr}\left( \bigcup_{k=0}^{i-1} L^k \right) \nonumber
\end{equation}
where $D_{min}$ is the set of all minimal elements in $D$.
\end{definition}

The first level set $L^0$ is simply the set of all minimal elements in $D$. The subsequent level sets are defined by taking the union of all the previous level sets and taking the cover of this union. Intuitively, each level set captures the neighborhood of the previous level sets in $D$.



\subsection{Partially ordered Markov models:}

Consider a finite set of random variables $\{ Z(s_1),\ldots,Z(s_n) \}$ indexed by location or "points" $$
D = \{s_1,\ldots ,s_n \}: n \in \{1,2,\ldots \}
$$

That is, we assume the existence of a directed acyclic graph $(D,F)$ and its corresponding poset $(D,\prec).$
Let $(D,F)$ be a finite, directed acyclic graph and its corresponding poset $(D,\prec).$ Consider $s \in D$ and recall the definition of cone $s$. Also, let the quantity $U_s$ denote any subsets of points not related to s. 
Formally:
$$
U_s \subset \{ u \in D: u \text{ and } s \text{ are not related } \}
$$
\begin{definition}[POMM]
Then $\{ Z(s): s \in D \}$ is said to be a partially ordered Markov model (POMM) if , for all $s \in D$ and any $U_s$
\begin{equation}
P(Z(s)|Z(\text{ cone } s), Z(U_s)) = P(Z(s)|Z(\text{ adjl } s)
\end{equation}
\end{definition}
\begin{proposition}\label{eq:result1}[Joint Distribution]
Let $(D,F)$ be a directed acyclic graph with no singleton points and let $(D, \prec)$, be its associated poset. Suppose that $\{Z(s): s \in D\}$ is a POMM. Then 
\begin{align}
P(Z(D)) &= P(Z(L^0)) \prod_{k=1}^m \prod \{ P(Z(u))| Z(\text{ adjl } u): u \in L^k \} \\
&= P(Z(L^0)) \prod \{ P(Z(u))| Z(\text{ adjl } u): u \in D 
\setminus L^0 \}
\end{align}
where $L^0, L^1, \ldots L^m$ are the level sets as defined previously.
\end{proposition}







Result 1 relates the probability of a random variable defined on a poset to the probabilities of its restrictions to the lower level sets of the poset.

The result states that the probability of $Z$ on the entire poset $D$ can be expressed as a product of the probabilities of $Z$ restricted to the level sets $L^0, L^1, \ldots, L^m$ of the poset, where $L^0 = D_{\text{min}}$ is the set of minimal elements of $D$, and $L^k$ is the set of elements of $D$ that are not in any of the previous level sets $L^0, L^1, \ldots, L^{k-1}$ and whose immediate predecessors are all in the union of the previous level sets $\bigcup_{i=0}^{k-1} L^i$.

The first part of the result states that the probability of $Z$ on $D$ is equal to the product of the probability of $Z$ on $L^0$ and the conditional probabilities of $Z$ on the elements of each subsequent level set $L^k$, given the values of $Z$ on their immediate predecessors. This can be seen as a form of the chain rule of probability, where the joint probability of $Z$ on $D$ is decomposed into a product of conditional probabilities.

The second part of the result simplifies the product by noting that the conditional probabilities of $Z$ on the elements of $D\setminus L^0$ are determined by the values of $Z$ on their immediate predecessors, which are all in $L^0$ or $D\setminus L^0$. Therefore, the product can be simplified to the product of the probability of $Z$ on $L^0$ and the conditional probabilities of $Z$ on the elements of $D\setminus L^0$ given the values of $Z$ on their immediate predecessors in $D\setminus L^0$. This simplification reduces the number of terms in the product and makes the computation of the joint probability of $Z$ on $D$ more efficient.




\newpage







\section{Application to the SST matrix}



In this section, I will introduce a matrix that displays the Strong Stochastic Transitivity (SST) property. The aim of this section is to model the probability of a player winning a match in a tournament against another one. To achieve this goal, we need to ensure that the probabilities are arranged in a consistent manner. Specifically, we want to ensure that if Player A is stronger than Player B, and Player B is stronger than Player C, then Player A must be stronger than Player C. This is a well-known mathematical concept known as transitivity, and it is essential to impose it on the probabilities of victory between the players. For instance, if we know that Djoković is stronger than Medvedev and Medvedev is stronger than Kyrgios, then we can infer that Djoković must be stronger than Kyrgios. By using the SST property, we can guarantee that the probabilities of victory reflect this logical relationship, which enhances the clarity and coherence of our model.
After having introduced the SST property, we want to build upon the definitions of Section(1), and re-express the SST matrix within the POMMs' framework. This new definition will allow us to have a coherent and tractable framework to express the joint probability distribution of such ordered probabilities and, ultimately, to perform inference.

\subsection{Defining matrix P (SST matrix)}
The matrix under consideration, that is $P$, is a collection of victory probabilities among $K$ entities, which could represent players or also groups of players. The matrix is denoted by $\underset{K \times K}{P}$, where $K$ is the number of players/ group of players taken into consideration. 

\begin{center}
$$P = 
\left(\begin{array}{cccc}p_{1,1} & p_{1,2} &  ... & p_{1,K} \\ p_{2,1} & p_{2,2} & ... & p_{2,K} \\ \vdots & \vdots & \vdots & \vdots \\ p_{K,1}& p_{K,2 }& ... & p_{K,K}\end{array}\right)$$
\end{center}



Each element $p_{i,j}$ in the matrix $P$ represents the probability of player $i$ winning over player $j$ in a tennis match, where draws are not permitted. Therefore, it must be the case that $p_{i,j} + p_{j,i} = 1$ in order to satisfy the requirements for a valid probability. It follows that $p_{j,i}$ can be expressed as $1 - p_{i,j}$. The lower triangular entries of matrix $P$ can be determined from the upper triangular entries. Consequently, our focus is on modelling the upper triangular part of $P$.
 
Without loss of generality, we can assume that player/team 1 is the strongest, and player/team $K$ is the weakest.

From this assumption, it follows that the elements in the upper triangular part of the matrix must remain above $0.5$ to maintain the assumption of monotonicity in the probabilities. Violation of this assumption could lead to a contradiction where a weaker player/team has a higher probability of winning than a stronger one. For instance, if $p_{1,2} = 0.4 \leq 0.5$, then $p_{2,1} = 0.6$ would imply that player 1 is weaker than player 2, which is contradictory with our baseline assumption. Furthermore, we set the main diagonal of matrix $P$ to $0.5$ for teams and $0$ for individual players.

Therefore, we must constrain the probabilities as follows:

\begin{itemize}
\item The probabilities must increase monotonically as the index of the columns $j$ increases;
\item The probabilities must decrease monotonically as the index of the rows $i$ increases.
\end{itemize}
The matrix $P$ with the described modification will look like this: 
\begin{center}
$$
\left(\begin{array}{ccccccc} 0.5 & \leq & p_{1,2} & \leq &  ... & \leq & p_{1,K} \\  &  & \vertgeq &  & ... &  & \vertgeq \\
1 - p_{1,2} & \leq & 0.5 & \leq & ... & \leq & p_{2,K} \\ \vdots & \vdots & \vdots & \vdots &  ... & \vdots& \vdots \\ 1 - p_{1,K}& \leq & 1 - p_{2,K}& \leq & ... & \leq & 0.5\end{array}\right)
$$
\end{center}


\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth,natwidth=1386,natheight=1000]{/Users/lapo_santi/Desktop/Nial/project/POMMs/IMG_0EE4E8AB347D-1.jpeg}
\caption{Dag representation of the Poset imposed onto the SST matrix}
\label{fig:DAG}
\end{center}
\end{figure}

\subsection{Defining a Poset over $P$}
Having defined $P$, now we want to re-define it within a finite Poset framework to obtain $(P, \leq)$. Let $(i,j)$ and $(p,q)$ be two elements of $P$ and define the binary relation $\leq$ on $P$ as 
\begin{equation}
(p,q) \leq (i,j)  \iff p \leq i \text{ and } q \leq j
\end{equation}

Now, $(P, \leq)$ is clearly a Poset since it satisfies the three properties of Definition \eqref{def:Poset}. We represent the corresponding directed acyclic graph $(P,F)$, where $F$ is the set of directed edges between vertices in Figure (\ref{fig:DAG}) by using the definition of Adjacent Lower Neighborhood that we introduced before.



\begin{definition}[Cone over $P$]In this case, for any $(i,j) \in D$ the cone of $(i,j)$ is the set:
\begin{equation}
\text{cone } (i,j)  = \{(i,j-1), \ldots (i,i), (i+1,j), \dots,(K,j) \}
\end{equation}
\end{definition}
\begin{definition}[$\overline{\text{cone }} (i,j)$]
In this case, for any $(i,j) \in P$ the closure of the cone of $(i,j)$ is the set:
\begin{equation}
\overline{\text{cone }} (i,j)  = \{(i,j), \ldots (i,i), (i,j), \dots,(K,j) \}
\end{equation}
\end{definition}

\begin{definition}[The adjacent lower neighborhood of $(i,j)$]
In this case, for any $(i,j) \in P$ the adjacent lower neighborhood of $(i,j)$ is the set:
\begin{equation}
\text{adjl } (i,j)  = \{(i,j-i), (i+1,j)\}
\end{equation}
\end{definition}

\begin{definition}[closure of $\text{adjl } (i,j)$]
In this case, for any $(i,j) \in P$ the closure of $\text{adjl } (i,j)$ is the set:
\begin{equation}
\overline{\text{adjl }} (i,j)  = \{(i,j-i), (i+1,j), (i,j)\}
\end{equation}
\end{definition}

\begin{definition}[$D_{min}^P$]
In this case, the \textit{minimal element} denoted by $D_{min}$ is such that
\begin{equation}
P_{min}  = \{(i,j) \in P: i=j\}
\end{equation}
namely the main diagonal of the $P$ matrix. 
\end{definition}
\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth,natwidth=1921,natheight=1000]{/Users/lapo_santi/Desktop/Nial/project/POMMs/level_sets.jpeg}
\caption{Visual Display of the level sets $L^k: k=0,\ldots,4$}
\label{fig:level_sets}
\end{center}
\end{figure}


The \textit{level sets} of $P$, in this case, corresponds to the diagonals above the main one.


\subsection{Partially ordered Markov Models applied to P}
\bigskip
Considering the finite set of random variables $\{P_{1,1},\dots, P_{K,K} \}$ indexed by locations where 
$$D\equiv \{(1,1), \dots (K,K)\}$$

Having showed the existence of a directly acyclic graph $ (P,F)$ and its corresponding poset $(P, \leq)$, we can write down 
\begin{align}
P(P_{ij}| Z(\text{cone } i,j)) &= P(P_{ij}| P(\text{adl } i,j))\\
P(P_{ij}| P_{i+1,j}, P_{i,j-1})
\end{align}

Now, exploiting Proposition \eqref{eq:result1}, we can write:
\begin{equation}
P(P(D)) = \prod_{i=1}^K\prod_{j=i}^K P(P_{ij}| P_{i+1,j}, P_{i,j-1})
\end{equation}


\begin{comment}
\section{Truncated specifications: hard constraint}

The modeling choice is a truncated beta distribution. The truncation helps us in various respects:
\begin{enumerate}
\item \textit{Parsimony}: The truncation in correspondence of the maximum value of the two adjacent lower neighbors of a given entry $p_{i,j}$ gives us the possibilityof obtaining the result SST property with just one parameter, namely the variance of the beta.
\item \textit{Stability}: The beta parameters' space is less regular than one might think. First, when one expresses $\alpha, \beta$ in terms of expected value $\mu$ and variance $\sigma^2$ we need to bear in mind that $\sigma^2 < \mu(1-\mu)$ to have $ \alpha, \beta > 0 $. A second problematic aspect is that if the prior process reaches the value $p_{i,j}^{(s)} = 1$ for some values $i,j$, the subsequent value will be not defined because the support of a beta distributed variable is $[0,1]$, but by the constraint of the process itself $p_{i,j}^ {(s+1)}> 1$, which is not defined. For this reason, it is necessary to truncate to $\approx .9999$ each variable of the process.
\item \textit{Consistency}: The truncation at $M$ ensures that $100\%$ of the sampled matrices will respect the desired SST property.
\end{enumerate}






\subsection{Truncated Beta Distribution with fixed variance}
$$P(p_{i,j}|\text{adjl}(p_{i,j})) = \text{Beta}\left(\alpha, \beta; 0, \mathbb{I}(p_{ij} \geq \max_{z \in \text{adjl}(p_{i,j})} z)\right)$$
where $\text{Beta}(\alpha, \beta)$ denotes the beta distribution with parameters $\alpha$ and $\beta$, and $1(\max_{z \in \text{adjl}(p_{i,j})} z)$ is the truncation at the maximum of the adjacent lower neighbourhood, from now on denoted by M.  The parameters $\alpha$ and $\beta$ are set such that:
\begin{align}
\mathbb{E}[p_{i,j}|\text{adjl}(p_{i,j})] &= M\\
Var[p_{i,j}|\text{adjl}(p_{i,j})] &= \lambda
\end{align}

\subsection{Truncated Beta Distribution with variance varying with column and row index}
$$P(p_{i,j}|\text{adjl}(p_{i,j})) = \text{Beta}\left(\alpha, \beta; 0, \mathbb{I}(p_{ij} \geq M_z)\right)$$ where $\text{Beta}(\alpha, \beta)$ denotes the beta distribution with parameters $\alpha$ and $\beta$, and $1(\max_{z \in \text{adjl}(p_{i,j})} z)$ is the truncation at the maximum of the adjacent lower neighborhood, from now on denoted by M.  The parameters $\alpha$ and $\beta$ are set such that:
\begin{align}
\mathbb{E}[p_{i,j}|\text{adjl}(p_{i,j})] &= M \\
Var[p_{i,j}|\text{adjl}(p_{i,j})] &= \lambda * (j-i)
\end{align}

where $(j-i)$ is increasing as we get closer to the upper-right corner. The following matrix containing the values is an example:
\begin{table}
\begin{center}\begin{tabular}{cccc}0 & 1 & 2 & 3 \\-1 & 0 & 1 & 2 \\-2 & -1 & 0 & 1 \\-3 & -2 & -1 & -0\end{tabular} \caption{A $4\times4$ matrix where the element $A_{i,j} = (j-i)$}
\end{center}
\label{defaulttable}
\end{table}


\subsection{Truncated Beta Distribution with variance specified for each $(ij)$}



$$P(p_{i,j}|\text{adjl}(p_{i,j})) = \text{Beta}\left(\alpha, \beta; 0, \mathbb{I}(p_{ij} \geq M_z)\right)$$

where $\text{Beta}(\alpha, \beta)$ denotes the beta distribution with parameters $\alpha$ and $\beta$, and $1(\max_{z \in \text{adjl}(p_{i,j})} z)$ is the truncation at the maximum of the adjacent lower neighbourhood, from now on denoted by M.  The parameters $\alpha$ and $\beta$ are set such that:
\begin{align}
\mathbb{E}[p_{i,j}|\text{adjl}(p_{i,j})] &= M\\
Var[p_{i,j}|\text{adjl}(p_{i,j})] &= \lambda_{ij}
\end{align}
where $\lambda_{ij}$ is varying across the elements, guaranteeing maximal flexibility but possibly creating identifiability issues.






\end{comment}





















\newpage


\begin{comment}
\section{Stick-Breaking prior process}

In this section, we define a generative process for the SST matrix using the POMM framework. We consider the above-defined level sets $L^1, \ldots, L^K$ for a $K-$ dimensional matrix $P$ and we specify a hyperprior distribution process over the POMM one. In particular, we will resort to a modified version of the stick-breaking prior process to create matrices which satisfy the SST property. 


\subsection{General definition of the Stick-Breaking prior process}
\bigskip
A Stick-Breaking prior process, conceptually, involves repeatedly breaking off and discarding a random fraction (sampled from a Beta distribution) of a "stick" that is by default of length 1. 
The distribution drawn is discrete with probability 1. In the stick-breaking process view, we explicitly use the discreteness and give the probability mass function of this (random) discrete distribution as:

$$ f(\theta) = \sum_{k=1}^{\infty} \beta_k \cdot \delta_{\theta_k}(\theta) $$

where $\delta_{\theta_k}$ is the indicator function which evaluates to zero everywhere, except for $\delta_{\theta_k}(\theta_k)=1$. Since this distribution is random itself, its mass function is parameterized by two sets of random variables: the locations $\left\{\theta_k\right\}_{k=1}^{\infty}$ and the corresponding probabilities $\left\{\beta_k\right\}_{k=1}^\infty $. 

The probabilities $\beta_k$ are given by a procedure resembling the breaking of a unit-length stick (hence the name).
\begin{equation}
\beta_k = \beta^{\prime}_k\cdot\prod_{i=1}^{k-1}\left(1-\beta^{\prime}_i \right)
\end{equation}

where $\beta^{\prime}_k $are independent random variables with the beta distribution $\operatorname{Beta}(1,c)$. The resemblance to 'stick-breaking' can be seen by considering $\beta_k$ as the length of a piece of a stick. We start with a unit-length stick and in each step, we break off a portion of the remaining stick according to $\beta^{\prime}_k$ and assign this broken-off piece to $\beta_k$. The formula can be understood by noting that after the first $k-1$ values have their portions assigned, the length of the remainder of the stick is $\prod_{i=1}^{k-1}\left(1-\beta^{\prime}_i\right)$ and this piece is broken according to $\beta^{\prime}_k$> and gets assigned to $\beta_k$

The smaller $c$ is, the less of the stick will be left for subsequent values (on average), yielding more concentrated distributions. 



\subsection{Application to the SST case}

In the context of the specific case, the level sets of the POMM are defined as the locations $\theta_k$, and a stick-breaking prior is imposed on them through truncated Beta distributions. The first step is to define the $\operatorname{Beta}(1,c)$ distributed random variables, namely  $b^i$: we do that in Equation \eqref{eq:beta_dist}. Then, $p_i$ values are generated according to the stick-breking process as in Equation \eqref{eq:stick_breaking}. The step in equation \eqref{eq:mapping} maps these proportions from the $[0,1]$ interval to the $[0.5, \beta^0_{\max}]$ one, via the function $f^{\beta^0_{\max}}$, which is nothing but a linear transformation of the values. Moreover, since we want to induce an increasing behaviour in the boundaries for the $p_{ij}$ we take the cumulative sum of the simulated $p_i$. Finally, in Equation \eqref{eq:truncation} we simulate the entries of the POMM from a $\operatorname{Beta}(1,1)$, where we have different truncations depending on the level set, meaning those entries $\{p_{ij} \in P :(j-i) = k\} \quad \text{ for } k = 1, \ldots, K $.

Here is the full model specification of a $\operatorname{POMM}(c,\beta^0_{\max},K)$ process:

\begin{align}
b_i &\sim \operatorname{Beta}(1,c) \quad &\text{ for } i = 1,\ldots,K \label{eq:beta_dist}\\
p_i &\sim b_i \prod_{j=1}^{i-1} (1 - b_j) \quad &\text{ for } i = 1,\ldots,K \label{eq:stick_breaking}\\
\beta^0_i &= f_{[0,1]\rightarrow[0.5, \beta^0_{\max}]} \left( \sum_{j=1}^i p_j \right) \quad &\text{ for } i = 1,\ldots,K \label{eq:mapping}\\
p_{ij} &\sim \operatorname{Beta}(1,1) \mathbb{I}(\beta^0_{k-1} <L^k < \beta^0_{k+i} ) \quad &\text{ for } (j-i) = k, \quad \text{ for } k= 1,\ldots,K \label{eq:truncation}
\end{align}

where $c$ is the concentration parameter, $\beta^0_1 = 0.5$ and $\beta^0_{\max}$ is the maximum value that the entries $p_{ij}$ can take.

To investigate the role of $c$, we have represented in Figure \ref{fig:cvalues} the evolution of the process $\beta^0_i \quad \text{ for } i = 1,..., K$ according to different $c$ values. We have done the same for the hyper-parameter $\beta^0_{\max}$ in Figure \ref{fig:differentbetavalues}. Finally, the resulting stick-breaking prior can be visualized in Figure \ref{fig:POMMpriormatrix} an example of the simulated matrix given $K=10,c=3,\beta^0_{\max} = .7$.

The overall joint probability distribution is as follows:
\begin{align}
p(p_{ij} \in P|c,\beta^0_{\max})&= \prod_{k=1}^K \prod_{ij: (j-i)=k}^{((K-1)*K)/2} \frac{1}{\operatorname{Beta}(1,1)}\mathbb{I}(\beta^0_{k-1} < L^k < \beta^0_{k+i}) \\
 &\times \prod_{k=1}^K \left(\beta_k^0 \right)^{-1} \prod_{j=1}^{k-1} (1 - \left(\beta_j^0\right)^{-1}) \\ 
 &\times\prod_{k=1}^K \frac{1}{\operatorname{Beta}(1,c)} b_k (1-b_k)^{c-1}
\end{align}

where $\left(\beta_k^0\right)^{-1}= f_{[0.5, \beta^0_{\max}]\rightarrow [0,1]} \left( \Delta_{j=1}^{k} \beta_j^0 \right) $ where $\Delta$ computes the cumulative difference of the boundaries for the truncated beta distribution. 

In practice, we are first mapping back the observed truncation from $[0.5, \beta^0_{\max}]$ to the space $[0,1]$, and then take the inverse of the cumulative summation in \eqref{eq:mapping} using the $\Delta$ operator. 


Overall, the Stick-Breaking prior process provides a rigorous and coherent method for generating discrete distributions with a flexible and controllable parametrization 



\begin{figure}[htbp]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
\includegraphics[width=\textwidth,natwidth=400,natheight=330]{/Users/lapo_santi/Desktop/Nial/project/POMMs/Stick-Breaking prior/Differentcvalues.png}
\caption{The plot shows the evolution of the $\beta^0$ prior process for different $c$ values and $\beta^0_{\max} = 0.9 $. Every line corresponds to a different concentration parameter $c$, as shown in the legend.}
\label{fig:cvalues}
\end{minipage}\hfill
    \begin{minipage}{.5\textwidth}
        \centering
\includegraphics[width=\textwidth,natwidth=374,natheight=309]{/Users/lapo_santi/Desktop/Nial/project/POMMs/Stick-Breaking prior/differentbetavalues.png}
\caption{The plot shows the evolution of different $\beta^0$ prior processes. Every line corresponds to a different $\beta_{max}$ value, given a concentration parameter $c = 3$. }
\label{fig:differentbetavalues}
\end{minipage}
\hfill
    \begin{minipage}{\textwidth}
        \centering
\includegraphics[width=0.5\textwidth,natwidth=500,natheight=413]{/Users/lapo_santi/Desktop/Nial/project/POMMs/Stick-Breaking prior/Heatmap_pomm.png}
\caption{The plot shows a POMM matrix generated with parameters $K=10,c=3,\beta_{max} = 0.8$. The higher value is 0.79, which means that we are able to enforce the truncation as wanted. The parameter $c=3$ translates into a moderately concentrated matrix, where we do not have big differences in players' winning probability}
\label{fig:POMMpriormatrix}
\end{minipage}
 \end{figure}










\newpage

\section{Power Law prior process}

The Power Law process is used to generate a prior process that can produce a SST matrix with a desired degree of flexibility. The probability values in the SST matrix should increase in the columns and decrease in the rows. To achieve this effect, the Power Law process uses level sets, which correspond to the diagonals in the upper triangular part of the matrix. The entries within each level set are modelled as truncated beta distributions with parameters (1,1).

More specifically, let us denote the set of upper triangular entries of the matrix as $U = {(i,j) \in \mathbb{N}^2 : i \leq j}$. Then, we can use the notation $\mathbf{P}_{(i,j) \in U}$ to represent the upper triangular entries of the matrix $\mathbf{P}$. The probability density function of the entries in the upper triangular part of the matrix can be written as:

\begin{equation}
p(\mathbf{P}{(i,j) \in U}) = \prod{k=1}^K \prod_{j-i=k}^{|L^{(k)}|} \frac{1}{y^{(k+1)} - y^{(k)}} = \prod_{k=1}^K \left( \frac{1}{y^{(k+1)} - y^{(k)}} \right)^{|L^{(k)}|}
\end{equation}

where $|L^{(k)}|$ is the cardinality of the level set $L^{(k)}$, and $y^{(k)}$ is the truncation point of the beta distribution that corresponds to level set $k$.

The truncation generating process starts with the function $y = x^\alpha + 0.5 \in [0.5, \beta^0_{\max}]$, which is a monotonically increasing function that is linear for $\alpha = 1$, concave for $\alpha \in (0,1)$, and convex for $\alpha > 1$. The function is used to provide truncation points for the beta distributions.

To generate truncations, we take the inverse function of $y$, denoted as $f(y)^{\leftarrow} = (y - 0.5)^{(1/\alpha)}$. The support of $y$ is then found to be $[f(0.5)^{\leftarrow}, f(\beta^0_{\max})^{\leftarrow}] = [0, (\beta^0_{\max} - 0.5)^{(1/\alpha)}]$, which is then divided into $K$ segments of equal length $\Delta = ((\beta^0_{\max} - 0.5)^{(1/\alpha)} - 0) / K$. The segment endpoints are defined as $x_k = \Delta \times (k-1)$ for $k=1,\ldots,K+1$. The cumulative sum of the segment endpoints is then mapped back to $y$, resulting in $K$ truncation points $y^{(k)}$, which are defined as:

\begin{equation}
y^{(k)} = y^{(k)} = \left( \frac{(\beta^0_{\max} - 0.5)^{(1/\alpha)}}{K} \times (k-1) \right)^\alpha + 0.5
\end{equation}


\subsection{Specific case: main diagonal equal to 0.5}






Let us define once again $Y = L(\alpha)^k$. Then let us denote with $p^Y$ the pdf of $Y$ and with $p^\alpha$ the pdf of $\alpha$. We have that:
\begin{align}
\label{eq:pdfy}
p_Y(y^{(k)}|\mu, \sigma^2) &= \left| \frac{d L^{-1}(y^{(k)})}{d(y^{(k)})} \right| \cdot p^\alpha \left( L^{-1}(y^{(k)}) \right) \\
&= \left| \dfrac{2}{\ln\left(\frac{(k-1)}{K-1}\right)\left(2y^{(k)}-1\right)} \right| 
 \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left( -\frac{1}{2\sigma^2}  \left( \frac{\ln \left( y^{(k)} - 0.5 \right) -  \ln \left(  (\beta^0_{\max} - 0.5) \right)}{ \ln \left( \frac{(k-1)}{K-1} \right) }- \mu \right)^2 \right) } \nonumber \\
 & \mathbb{I}\left(\frac{\ln \left( y - 0.5 \right) -  \ln \left(  (\beta^0_{\max} - 0.5) \right)}{\ln \left( \frac{(k-1)}{K} \right) } > 0 \right) \nonumber
\end{align} 


$L$ is a deterministic transformation. Substituting $y$ and $\Delta$ one can find:
\begin{equation}
L^k(\alpha) = \left( \frac{(\beta^0_{\max} - 0.5)^{(1/\alpha)}}{K} \times (k-1) \right)^\alpha  + 0.5
\end{equation}


To find the inverse function of $L(\alpha)^k$, we need to solve for $\alpha$ in terms of $L$. We can start by isolating the term inside the parentheses. To simplify the notation we can set $y:= L(\alpha)^k$.

\begin{align*}
y&= \left( \frac{(\beta^0_{\max} - 0.5)^{(1/\alpha)}}{K} \times (k-1) \right)^\alpha + 0.5 \\
y - 0.5 &= \left( \frac{(\beta^0_{\max} - 0.5)^{(1/\alpha)}}{K} \times (k-1) \right)^\alpha \\
\end{align*}

Next, we can take the logarithm of both sides:

\begin{align*}
\ln \left( y - 0.5 \right) &= \alpha \ln \left( (\beta^0_{\max} - 0.5)^{(1/\alpha)}\times \frac{(k-1)}{K}   \right) \\
\ln \left( y - 0.5 \right) &= \alpha \left[ \frac{1}{\alpha} \ln \left(  (\beta^0_{\max} - 0.5) \right) + \ln \left( \frac{(k-1)}{K}   \right) \right] \\
\ln \left( y - 0.5 \right) &=    \ln \left(  (\beta^0_{\max} - 0.5) \right) + \alpha \ln \left( \frac{(k-1)}{K}   \right) \\
\ln \left( y - 0.5 \right) -  \ln \left(  (\beta^0_{\max} - 0.5) \right) &=    \alpha \ln \left( \frac{(k-1)}{K}   \right) \\
\frac{\ln \left( y - 0.5 \right) -  \ln \left(  (\beta^0_{\max} - 0.5) \right)}{\ln \left( \frac{(k-1)}{K} \right) } &=    \alpha  \\
\end{align*}
Therefore, the inverse function of $L(\alpha)^k$ is:

\begin{equation}
L^{-1}(y) = \frac{\ln \left( y - 0.5 \right) -  \ln \left(  \beta^0_{\max} - 0.5 \right)}{\ln \left( \frac{(k-1)}{K} \right) }
\end{equation}



Now we can take the derivative with respect to $y$:

\begin{equation} 
 \frac{d L^{-1}(y)}{d(y)}  = 
\dfrac{2}{\ln\left(\frac{(k-1)}{K}\right)\left(2y-1\right)}
\end{equation}


Now we have all the ingredients to derive the density of $L(\alpha)^k$ by transforming the probability density of $\alpha$, namely $p^\alpha(\alpha) = Normal(\mu,1)\mathbb{I}(\alpha>0)$.

Let us define once again $Y = L(\alpha)^k$. Then let us denote with $p^Y$ the pdf of $Y$ and with $p^\alpha$ the pdf of $\alpha$. We have that:
\begin{align}
\label{eq:pdfy}
p_Y(y^{(k)}|\mu, \sigma^2) &= \left| \frac{d L^{-1}(y^{(k)})}{d(y^{(k)})} \right| \cdot p^\alpha \left( L^{-1}(y^{(k)}) \right) \\
&= \left| \dfrac{2}{\ln\left(\frac{(k-1)}{K-1}\right)\left(2y^{(k)}-1\right)} \right| 
 \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left( -\frac{1}{2\sigma^2}  \left( \frac{\ln \left( y^{(k)} - 0.5 \right) -  \ln \left(  (\beta^0_{\max} - 0.5) \right)}{ \ln \left( \frac{(k-1)}{K-1} \right) }- \mu \right)^2 \right) } \nonumber \\
 & \mathbb{I}\left(\frac{\ln \left( y - 0.5 \right) -  \ln \left(  (\beta^0_{\max} - 0.5) \right)}{\ln \left( \frac{(k-1)}{K} \right) } > 0 \right) \nonumber
\end{align} 

Therefore the final prior distribution on the $p_{ij}$ will be:
\begin{align}
p(p_{ij}, L^k | \mu) = p(p_{ij}|L^k, \mu) p(L^k | \mu)
\end{align}
where 
\begin{align}
p(L^k | \mu) &\text{ is like in \eqref{eq:pdfy}} \\
p(p_{ij}|L^k, \mu) &= Beta(1,1) \mathbb{I} \left( p_{ij} : j-i= k \in (L^{k-1},L^{k}) \right)
\end{align}




\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth,natwidth=374,natheight=309]{/Users/lapo_santi/Desktop/Nial/project/POMMs/power-law prior/boundaryevolution.png}
\caption{ }
\label{ }
\end{center}
\end{figure}



\begin{figure}[htbp]
    \centering
    \begin{minipage}{\textwidth}
        \centering
\includegraphics[width=.5\textwidth,natwidth=374,natheight=309]{/Users/lapo_santi/Desktop/Nial/project/POMMs/power-law prior/alpha1_10.png}
        \caption{alpha1\_10.png}
        \label{fig:boxplotnij}
    \end{minipage}\hfill
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=.5\textwidth,natwidth=374,natheight=309]{/Users/lapo_santi/Desktop/Nial/project/POMMs/power-law prior/alpha1.png}
        \caption{alpha1.png}
        \label{fig:densityplotnij}
    \end{minipage}\hfill
    \vskip\floatsep
    \begin{minipage}{\textwidth}
        \centering
\includegraphics[width=.5\textwidth,natwidth=374,natheight=309]{/Users/lapo_santi/Desktop/Nial/project/POMMs/power-law prior/alpha1_5.png}
        \caption{alpha1\_5.png}
        \label{fig:boxplotyij}
    \end{minipage}
 \end{figure}

\end{comment}
\newpage



\section{Full model specification}

The following Bayesian model is used to describe and analyze pairwise data, with the specific aim to identify clusters of points with similar connectivity patterns. 

The model uses a Poisson distribution to model the number of blocks, a Dirichlet-multinomial distribution to model the distribution of nodes' assignment across blocks, and a binomial distribution to model the distribution of edges within blocks. Additionally, the model includes a POMM process to model the probability of edge formation between nodes within blocks.

The goal of the model is to estimate the number of blocks, the distribution of nodes across blocks, and the probability of edge formation between nodes within blocks, given observed network data. The Bayesian approach allows for uncertainty in these estimates and provides a framework for incorporating prior knowledge and updating beliefs as new data becomes available.

\subsection{Simple Model specification}
This is a model for pairwise count data. We explicitly model the results of the interactions between two individuals $i$ and $j$. Given $N$ observations, the likelihood is 
\begin{align}
p(y| z, P, K) &= \prod_{i =2 }^{N-1} \prod_{j =i}^{N} p(y_{ij} | z, P, K) \\ 
&= \prod_{i =2 }^{N-1} \prod_{j =i}^{N}  {n_{ij} \choose y_{ij}} p_{z_i, z_j}^{y_{ij}}(1- p_{z_i, z_j})^{n_{ij}-y_{ij}}
\end{align}

where $n_{ij}$ denotes the total number of interactions between the two individuals $i$ and $j$ and $y_{ij}$ is the number of successes of the individual $i$ in interacting with $j$. The probability of success is given by $p_{z_i, z_j}$ which consists of two parameters. The $K\times K$ matrix $P$ and the $N \times 1$ vector $z$.

The vector $z$ takes values over the discrete and finite set $\{ 1 , \ldots, K\}$, and it is an indicator variable such that if $z_i = k$ individual $i$ belongs to block $k$. 

The matrix $P$ contains the probabilities of success for individuals belonging to each possible blocks combination. For this reason $P$ is $K\times K$. Therefore, the parameter $p_{z_i, z_j}$ consists in the probability of success in an interaction between one individual belonging to block $z_i$ and another of block $z_j$. 

\subsection{Prior Specification}

This model has three parameters, and we put a prior on each of them.

Starting with $P$, we assume that its entries, namely $p_{k,k^\prime}$, are independent and identically $Beta(a,b)$ distributed random variable. By setting $a=b=1$ they collapse to a uniform distribution.
\begin{equation}
p_{k,k^\prime} \sim Beta(1,1) \quad \text{for } k,k^\prime = 1, \ldots,K
\end{equation}
Second, we assume that the $z_i$s are independent and identically drawn from a multinomial distribution with one trial and probability vector $(\theta_1, \dots, \theta_K)$. We can write then:
\begin{equation}
z_i| \boldsymbol{\theta} \sim \operatorname{Multinomial}(1,\boldsymbol{\theta}) \quad \text{for } i = 1, \ldots,N
\end{equation}


To have more flexibility in the blocks sizes, we put an hyper-prior on the $\theta_1, \dots, \theta_K$, assuming that they are drawn from a Dirichlet distribution with parameter the $K\times1$ vector $\boldsymbol{\gamma}$.


By marginalizing out $\theta$, following the common practice in the literature, we can express the marginal distribution of $z$ as:

\begin{equation} p(\mathbf{z}|\boldsymbol{\gamma}) = \frac{\Gamma(\sum_{k=1}^K \gamma_k)}{\prod_{k=1}^K \Gamma(\gamma_k)}\frac{\prod_{k=1}^K \Gamma(n_k+\gamma_k)}{\Gamma(\sum_{k=1}^K (n_k+\gamma_k)}
\end{equation}

where $n_k$ is the number of players assigned to block $k$.

Finally, we assume that the number of clusters $K$ follow a Poisson distribution $\operatorname{Poisson}(\lambda=1)$, subject to the condition $K>0$.




\subsection{POMM Prior}

In order to induce an ordering or ranking among the blocks, we introduce a hierarchical structure. Without loss of generality, we assume that block 1 has the highest probability of success when interacting with any other block, while block $K$ has the lowest probability of success. We require this ranking to be transitive, meaning that if block A has a higher probability of success when interacting with block B, and block B has a higher probability of success when interacting with block C, then block A must still be the preferred choice when interacting with block C. Mathematically, this can be expressed as $p_{k,h} > p_{k',h}$ when $k < k'$ for $h \notin { k, k' }$.

To achieve this effect, we impose three conditions: probabilities should increase in the columns, decrease in the rows, and be greater than or equal to 0.5 in the upper triangular matrix. To satisfy these conditions, we construct the following scheme.

\subsubsection{Level Sets}

We define the level sets, denoted by $L^{(k)}$, as the diagonals of the upper triangular matrix P. The main diagonal is referred to as level set 0, denoted by $L^{(0)}$. The diagonal above it is denoted by $L^{(1)}$, and so on up to $L^{(K-1)}$. Each level set $L^{(k)}$ is formally defined as:

\begin{equation}
L^{(k)} := { p_{ij} \mid j-i = k } \quad \text{for } k = 0, \ldots, K-1
\end{equation}

It is worth noting that the cardinality of each level set is given by $|L^{(k)}| = K - k$ for $k = 0, \ldots, K-1$.

\subsubsection{Truncation Process}

Requiring that probabilities increase in the rows and decrease in the columns is equivalent to ensuring that the level sets satisfy the condition:

\begin{equation}
\max(L^{(k)}) < \min(L^{(k+1)}) \quad \text{for } k = 0, \ldots, K-1
\end{equation}

To enforce this behavior, we employ an increasing truncation process controlled by a parameter $\alpha$, with an upper bound given by $\beta_{\max}$.

We consider a generic power-law function $y = x^\alpha + 0.5$, which governs the rate of increase in the truncation process. Setting $f(0) = 0.5$ ensures that $L^{(0)}$ is greater than or equal to 0.5, satisfying the transitivity condition. The function $y$ is monotonically increasing for $x > 0$. To generate the truncations, we partition $y$ effectively by dividing the interval into $K$ equal-sized segments. The segment endpoints are computed as $x_k = \Delta \times k$ for $k = 0, \ldots, K$, where $\Delta = \left( \left(\beta_{\max} - 0.5\right)^{(1/\alpha)} - 0 \right) / K$. Mapping the cumulative sum of the segment endpoints back to $y$ yields $K$ truncation points denoted by $y^{(k)}$, which are defined as:

\begin{equation}
\label{eq:truncations}
y^{(k)} = \left( \frac{(\beta_{\max} - 0.5)^{(1/\alpha)}}{K} \times k \right)^\alpha + 0.5 \quad \text{for } k = 0, \ldots, K
\end{equation}

Notice that $f(0) = 0.5$ and $f(K) = \beta_{\max}$ by construction.

These truncation points provide the upper and lower bounds for the entries within the corresponding level sets, thereby ensuring the desired hierarchy and transitivity in the ranking.

Mathematically we have that:
\begin{equation}
y^{(k)} < p_ij \in L^{(k)} < y^{(k+1)}\quad  k = 0, \ldots, K-1 
\end{equation}

\subsubsection{The POMM Prior}

Finally, we put a prior on the matrix $P$ with this new structure in place.
We assume that entries $p_{ij} \in L^{(k)} \mid (y^{(k)} + y^{(k+1)}$ are identically and independently distributed according to a $\operatorname{Uniform}(y^{(k)},y^{(k+1)}$. We also put a log-normal hyper-prior on $\alpha$ such that 
\begin{equation}
\alpha \sim \operatorname{lognormal}(\mu_\alpha, \sigma^2_\alpha)
\end{equation}
where $\mu_\alpha, \sigma^2_\alpha$ are specified according to the normal parametrisation of the lognormal and are fixed to 1 and 2 respectively.
Altogether, the POMM prior on P is the following:
\begin{align}
p_{ij} \in L^{(k)} \mid y^{(k)},y^{(k+1)} &\sim (y^{(k)},y^{(k+1)} \\
\alpha &\sim \operatorname{Lognormal}\left(\mu_\alpha, \sigma^2_\alpha\right)
\end{align}
and where the truncations $y^{(k)}$ are derived as in \eqref{eq:truncations}.



\subsubsection{The POMM Prior 2}


Finally, we put a prior on the matrix $P$ with this new structure in place.
We assume that entries $p_{ij} \in L^{(k)} \mid (y^{(k)} + y^{(k+1)}$ are identically and independently distributed according to a $\operatorname{Normal}(\mu^{(k)},\sigma^{2(k)})$, where $\mu^{(k)} = \frac{y^{(k)}+y^{(k+1)}}{2}$ which corresponds to the midpoint of the level set $L^{(k)}$, and $\sigma^{2(k)} = \left( y^{(k)}+y^{(k+1)} \right) \times S$, where $S$ is a parameter denoted as \textit{overlap}, which intuitively is proportional to the overlap in the distribution support of the level sets.   We also put a log-normal hyper-prior on $\alpha$ such that 
\begin{equation}
\alpha \sim \operatorname{lognormal}(\mu_\alpha, \sigma^2_\alpha)
\end{equation}
where $\mu_\alpha, \sigma^2_\alpha$ are specified according to the normal parametrisation of the lognormal and are fixed to 1 and 2 respectively.
Altogether, the POMM prior on P is the following:
\begin{align}
p_{ij} \in L^{(k)} \mid y^{(k)},y^{(k+1)} &\sim \operatorname{Normal}\left(\mu^{(k)},\sigma^{2(k)}\right) \mathbb{I}(0.5, \beta_{\max}) \\
\alpha &\sim \operatorname{Lognormal}\left(\mu_\alpha, \sigma^2_\alpha\right)\\
S \sim &\sim \operatorname{Lognormal}\left(\mu_S, \sigma^2_S \right)
\end{align}
and where the truncations $y^{(k)}$ are derived as in \eqref{eq:truncations}.


Insert here the plot for 3 different overlap values





\begin{figure}[htbp]
    \centering
    \subfigure[$\alpha=1, S=0.2$]{%
        \includegraphics[width=.33\textwidth,natwidth=800,natheight=524]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Densityplot_5_Level_sets_alpha__1__overlap__0.2__diag__0.5.png}%
        \label{fig:overlap0.2}%
    }\hfill
    \subfigure[$\alpha=1, S=0.5$]{%
        \includegraphics[width=.33\textwidth,natwidth=800,natheight=568]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Densityplot_5_Level_sets_alpha__1__overlap__0.5__diag__0.5.png}%
        \label{fig:overlap0.5}%
    }\hfill
    \subfigure[$\alpha=1, S=0.8$]{%
        \includegraphics[width=.33\textwidth,natwidth=800,natheight=568]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Densityplot_5_Level_sets_alpha__1__overlap__0.8__diag__0.5.png}%
        \label{fig:overlap0.8}%
    }
    \caption{Densities for $K=5$ Level Sets, for different $S$ values and $\alpha=1$; the main diagonal is set to 0.5, and its collapsed density is not reported.}
    \label{fig:all_images}
\end{figure} 

\subsection{Some notes on the overlap}

The overlap between two distributions with equal variances when \sigma^2 = \sigma





\begin{comment}

We denote the nodes in this setting as $1, \dots, n$. We define the model as follows:

\begin{itemize}
\item Let $K$ follow a Poisson distribution $\operatorname{Poisson}(\lambda=1)$, subject to the condition $K>0$.
\item Let $\theta_1, \dots, \theta_K$ be drawn from a Dirichlet distribution with parameter vector $\boldsymbol{\gamma}$.
\item Let $z_i$ be independently and identically drawn from a multinomial distribution with one trial and probability vector $(\theta_1, \dots, \theta_K)$.
\item Let $P = \{p_{i,j}\} \sim \operatorname{POMM}(\alpha,\beta^0_{\max},K)$.
\item Let $y_{i,j}$ follow a binomial distribution with parameters $n_{ij}$ and $p_{z_i, z_j}$, subject to the condition $n_{ij} > 0$.
\end{itemize}

Here, $z_i$ takes values in ${1, \dots, K}$. The likelihood function can be specified as:

\begin{equation}
p(\textbf{y} | p, z) = \prod_{i<j}^{n-1} {n_{ij} \choose y_{ij}} p_{z_i, z_j}^{y_{ij}}(1- p_{z_i, z_j})^{n_{ij}-y_{ij}}
\end{equation}

To specify the prior distribution, we start with the prior on $z$ following a multinomial distribution. We have $n$ players to assign to $K$ different labels, by replacing the extracted labels after each draw. We denote the variable which is the extracted label $i \in \{ 1, ..., K \}$ as $Z_i$, and denote as $\theta_i$ the probability that a given extraction will be of label $i$. The joint probability of $Z_1,\ldots,Z_n$ will be: 
\begin{align}
f(z_1,\ldots,z_n,\theta_1, \ldots, \theta_K ) & {} = \Pr(Z_1 = z_1,\ldots, Z_n = z_n) \\
& {} = \begin{cases} { \displaystyle {n! \over n_1!\cdots n_K!}\theta_1^{n_1}\times\cdots\times \theta_K^{n_K}}, \quad &
\text{when } \sum_{i=1}^K n_i=n \\  \\
0 & \text{otherwise,} \end{cases}
\end{align}

where $n_k = \sum_{i=1}^N \mathbb{I}(z_i = k) \quad \text{ for } k =1,\ldots,K$
for non-negative integers $z_1, ..., z_K$.

The probability mass function can be expressed using the gamma function as:


\begin{equation}
p(z_1, \ldots , z_n| \theta_1, \ldots, \theta_K ) = \frac{\Gamma(\sum_i n_i + 1)}{\prod_i \Gamma(n_i+1)} \prod_{i=1}^K \theta_i^{n_i}
\end{equation}

Here, $n_i$ represents the number of nodes/players allocated to block $i$. On $\theta$, the Dirichlet prior has the following form:

\begin{equation}
f \left(\theta_1,\ldots, \theta_{K}; \gamma_1,\ldots, \gamma_K \right) = \frac{1}{\mathrm{B}(\boldsymbol\gamma)} \prod_{i=1}^K \theta_i^{\gamma_i - 1}
\end{equation}

By marginalizing out $\theta$, following the common practice in the literature, we can express the marginal distribution of $z$ as:
\begin{equation}
p(\textbf{y} | p, z) = \prod_{i<j}^{n-1} {n_{ij} \choose y_{ij}} p_{z_i, z_j}^{y_{ij}}(1- p_{z_i, z_j})^{n_{ij}-y_{ij}}
\end{equation}
\begin{equation} p(\mathbf{z}|\boldsymbol{\gamma}) = \frac{\Gamma(\sum_{i=1}^k \gamma_i)}{\prod_{i=1}^k \Gamma(\gamma_i)}\frac{\prod_{i=1}^k \Gamma(n_i+\gamma_i)}{\Gamma(\sum_{i=1}^k (n_i+\gamma_i))}
\end{equation}

where $\mathbf{z} = (z_1, z_2, \dots, z_k)$ is a vector of labels for $k$ different categories, $\boldsymbol{\gamma} = (\gamma_1, \gamma_2, \dots, \gamma_k)$ is a vector of hyperparameters, and $\Gamma(\cdot)$ is the Gamma function.
For the distribution of $P$ we refer to the previous section in which we have detailed the specification. 

Finally, we specify the prior distribution for the parameter $K$ as:
\begin{equation}
p(K| \lambda =1) = \frac{1^K e^{-1}}{K!} =  \frac{1}{e\cdot K!} \propto \frac{1}{ K!}
\end{equation}

\section{Simulating synthetic data: code description}


The code \texttt{simulating\_tournament\_test} simulates a tournament with multiple matches between players who belong to different clusters. The number of clusters is simulated from a truncated Poisson distribution, and each player within a cluster is assigned to one of the possible clusters using a Dirichlet multinomial distribution. Then, for each cluster pair, the probability of a player from one cluster winning against a player from another cluster is calculated using POMM process. Finally, a given number of matches between random pairs of players is simulated, where the number of victories of each player is also simulated based on a binomial distribution.

More specifically, the function takes the following arguments:
\begin{itemize}
\item \texttt{n}: the total number of players in the tournament.
\item $\boldsymbol{\gamma}$: a parameter which controls the size of the blocks
\item $c$: a parameter which controls the dispersion of the probabilities within the matrix $P$
\item $\beta_{\max}$: a parameter which controls the probability of victory of the strongest player.
\item \texttt{min\_clust} and \texttt{max\_clust}: the minimum and the maximum number of clusters that can be created using a truncated Poisson distribution.
\item \texttt{M}: the number of matches to be simulated.
\item \texttt{n\_ij\_max}: the maximum number of games that can be played between any two players withi the tournament.
\end{itemize}

\subsection{Simulating $K$}\bigskip

As we already said, $K$ is simulated according to a truncated Poisson, i.e., 
\begin{equation} 
K \sim Poisson(\lambda=1) \mathbb{I}(\texttt{min\_clust}< K < \texttt{max\_clust}) 
\end{equation} distribution with parameter. As for \texttt{min\_clust}, \texttt{max\_clust}, they are input under the choice of the user, to better control the number of clusters. In order to sample from the truncated poisson we use inverse sampling:
\begin{align}
U &\gets Unif (0,1) \nonumber \\ 
K &\gets F^{-1}(((F(b, \lambda) - F(a, \lambda))*u+ F(a, \lambda)),\lambda))
\end{align}
where $F$ is the CDF of a Poisson distribution with parameter $\lambda$ and $a,b$ are respectively the lower and the upper truncations. In our specific case we have $\lambda =1, a = \texttt{min\_clust}, \texttt{max\_clust}$
Also in the following parts, when we will need to simulate a truncated Beta distribution, we will use the same method.

\subsection{Simulating $z|K$}\bigskip
After obtaining $K$, each player in the tournament is assigned to a cluster using a Dirichlet multinomial distribution with a concentration parameter denoted by $\boldsymbol{\gamma}$. This is done under the assumption that cluster 1 is the strongest and cluster $K$ is the weakest. The parameter $\boldsymbol{\gamma}$ controls the expected size of the clusters, namely how many nodes are assigned to each label. Three cases are presented to explain the reasoning behind $\boldsymbol{\gamma}$, which are not exhaustive. The three cases are as follows:

\begin{itemize}
\item $\boldsymbol{\gamma} = 1 \quad \forall k \in {1,\ldots,K}$, where the expected size of each cluster is the same.
\item $\boldsymbol{\gamma} = \left[ 1,\ldots, K \right]$, where the expected size of the clusters is inversely proportional to the probability of victory;
\item $\boldsymbol{\gamma} = \left[ K, \dots, 1 \right]$, where the expected size of the clusters is directly proportional to the probability of victory.
\end{itemize}

Given $\boldsymbol{\gamma}$, the probability distribution $p$ is simulated using the gamma distribution:
\begin{equation}
p \gets \texttt{rgamma}(K,\boldsymbol{\gamma})
\end{equation}
This is an indirect way to sample from the Dirichlet distribution. Subsequently, the labels are sampled from the Multinomial distribution using the following code:
\begin{equation}\label{eq:simulating_multinomial}
z \gets \texttt{sample(1:K,N,prob=p/sum(p),rep=TRUE)}
\end{equation}
Here, $z$ is an $N-$dimensional vector, where rows index stand for the players' IDs and each entry corresponds to a label of the assigned cluster.
\subsection{Simulating $\operatorname{POMM}(c,\beta^0_{\max},K)$}
\bigskip
The code \texttt{simulating\_POMM\_stick\_breaking} implements the stick-breaking process as follows:

\begin{itemize}
\item It generates $K$ beta-distributed random variables $\texttt{b\_1}, \dots, \texttt{b\_K}$ with a concentration parameter $c$ using the function \texttt{rbeta(K, 1, c)}.
\item It computes the stick-breaking proportions $\texttt{p\_1, \dots, p\_K}$ using the formula $$\texttt{p\_i = b[i] * prod(1 - b[1:(i-1)])}$$.
\item It sums over the proportions, \texttt{p\_sum $\gets$ cumsum(p)} to ensure an increasing behaviour and maps them to a desired scale using the formula $$\texttt{beta\_0 = (beta\_{max} - 0.5) * (p\_{sum} - min(p\_{sum}))/(max(p\_{sum}) - min(p\_{sum})) + 0.5}$$
where $\texttt{beta\_{max}}$ is a user-defined parameter that sets the maximum value of $\texttt{beta}$, and $\texttt{p\_{sum}}$ is the cumulative sum of the proportions.
\item It generates the $\texttt{beta}$ values using the proportions as truncations with the function \texttt{sample\_beta\_trunc(n\_items,1,1,beta\_0[K-i], beta\_0[K-i+1])}, where \texttt{n\_items} is the number of $\texttt{beta}$ values to generate and $K-i$ and $K-i+1$ are the indices of the two corresponding proportions in $\texttt{beta\_0}$.
\item It puts everything together in a matrix and returns the result.
\end{itemize}


\subsection{Sampling the matches}\bigskip


The data frame is called \texttt{z\_players}, and it consists of two columns: \texttt{id} and \texttt{z}. The \texttt{id} column contains the values 1 to n (where n is the number of players), which are the players' IDs,  and the \texttt{z} column contains the cluster assignment for each player, which is determined by the previous step in which we have simulated the parameter $z$.

This code generates a data frame (\texttt{df}) with the columns \texttt{player\_1, player\_2, n\_ij, y\_ij}, and \texttt{p\_ij}. The variables \texttt{player\_1} and \texttt{player\_2} represent the players who compete against each other in a match, \texttt{n\_ij} is the number of games they play, \texttt{y\_ij} is the number of games won by \texttt{player\_1}, and \texttt{p\_ij} is the probability of \texttt{player\_1} winning a game against \texttt{player\_2}.

The code uses a while loop that runs \texttt{M} times, where \texttt{M} is the user-specified number of matches.  In each iteration of the loop, it samples two players (\texttt{pl\_1\_i} and \texttt{pl\_2\_i}) from a pool of players (\texttt{z\_players\$id}) based on their probability of being chosen (\texttt{aux2\$p}). \texttt{pl\_2\_i} is chosen from the remaining players who have not been chosen as \texttt{pl\_1\_i}.

In this specific case, we set \texttt{aux2\$p} to be inversely proportional to the size of the cluster each player belongs to, and instead proportional to the winning $p_ij$, to take into account the fact that stronger players play more, due to the knockout mechanisms which are prevailing in almost every tennis tournament. In another application, this probability could be uniform, or inversely proportional to $p_ij$ 

So, by saying that the probability of playing is inversely related to the cluster size, we are effectively associating to each player, not only with its cluster but also with the probability of being assigned to that cluster, which we discussed in the Dirichlet-Multinomial step. This probability can be found at \eqref{eq:simulating_multinomial}, namely \texttt{prob=p/sum(p)}. With this association, we build \texttt{aux2\$p}, a $n\times1$ vector which provides the probabilities for matches formation.

A new data frame (\texttt{matches\_i}) is then created with the selected players, and their respective blocks (\texttt{z\_1} and \texttt{z\_2}) are added to the data frame. The number of games they play (\texttt{n\_ij}) is sampled from a truncated Poisson distribution with a maximum value of \texttt{n\_ij\_max}. The probability of \texttt{player\_1} winning a game against \texttt{player\_2} (\texttt{p\_ij}) is calculated based on their blocks (\texttt{z\_1} and \texttt{z\_2}) using the pre-simulated matrix \texttt{p}.

The number of games won by \texttt{player\_1} (\texttt{y\_ij}) is then sampled from a binomial distribution with parameters \texttt{n\_ij} and \texttt{p\_ij}.

Finally, the information for the current match is added to the df data frame using the \texttt{rbind()} function. The loop continues until it has run M times. The full code is presented in Algorithm (\ref{alg:matches}).

\begin{algorithm}
\caption{Building Matches}\label{alg:matches}
\begin{algorithmic}
\State Initialize data frame \texttt{df} with columns \texttt{player\_1}, \texttt{player\_2}, \texttt{n\_{ij}}, \texttt{y\_{ij}}, \texttt{p\_{ij}}, where each column is filled with \texttt{NA} values.
\State Initialize $i$ to 0.
\While{$i < M$}
\State Sample player \texttt{pl\_1\_i} from \texttt{z\_players\$id} with probability \texttt{aux2\$p}.
\State Sample player \texttt{pl\_2\_i} from the remaining players in \texttt{z\_players\$id} with probability \texttt{aux2\$p[setdiff(z\_players\$id,pl\_1\_i)]}.
\State Create a new data frame \texttt{matches\_i} with columns \texttt{pl\_1\_i, pl\_2\_i, z\_1, z\_2, n\_{ij}, y\_{ij}}, where \texttt{z\_1} and \texttt{z\_2} are the blocks of the players.
\State Set \texttt{matches\_i\$z\_1} to the block of \texttt{pl\_1\_i} from \texttt{z\_players}.
\State Set \texttt{matches\_i\$z\_2} to the block of \texttt{pl\_2\_i} from \texttt{z\_players}.
\State Sample the number of games \texttt{n\_{ij}} from a truncated Poisson distribution with maximum value \texttt{n\_{ij\_max}}.
\State Calculate the probability of \texttt{pl\_1\_i} winning a game against \texttt{pl\_2\_i} as \texttt{p\_{ij}} based on their blocks \texttt{z\_1} and \texttt{z\_2} using a matrix \texttt{p}.
\State Sample the number of games won by \texttt{pl\_1\_i} as \texttt{y\_{ij}} from a binomial distribution with parameters \texttt{n\_{ij}} and \texttt{p\_{ij}}.
\State Add the current match information to $df$ using the \texttt{rbind()} function to create a new row with values \texttt{pl\_1\_i}, \texttt{pl\_2\_i}, \texttt{n\_{ij}}, \texttt{y\_{ij}}, \texttt{p\_{ij}}.
\State Increment $i$ by 1.
\EndWhile
\end{algorithmic}
\end{algorithm}



\section{ATP tennis dataset}


Our raw data are in a repository which can be found at \begin{center}\texttt{'https://pkgstore.datahub.io/sports-data/atp-world-tour-tennis-data'}\end{center}. If two players have played multiple times, they compare on different rows as we can see in Table(1). 

\begin{table}
\begin{center}\begin{tabular}{cccc} $\textbf{Winner}$ & $\textbf{Loser}$ \\Djockovic & Medvedev \\ \vdots & \vdots \\Djockovic & Medvedev  \\Djockovic & Medvedev \\ Medvedev & Djockovic \\\vdots & \vdots \\ Medvedev & Djockovic \\ Medvedev & Djockovic  \end{tabular} \caption{Raw data}
\end{center}
\label{Raw data}
\end{table}

To begin with, we select all rows with the first pair of players, regardless of the order in which they appear. Then, we construct these two new variables:
\begin{itemize}
\item $n_{games}$: the total number of games between player A and player B, which can be calculated as the sum of the number of victories of player A against player B and the number of victories of player B against player A. Basically, one counts the number of times these two names appear on the same row, irrespective of their order.
\item $n_{victories}$: the total number of victories of player A against player B, which can be calculated as the number of times player A appears before player B on the same row.
\end{itemize}

Then, I retain just the first pair observation, e.g. player A and player B, and disregard all the others. This process is repeated for all unique unordered pairs of players. In this way, I obtain a dataset storing, for each pair of players who have played at least once, all the relevant information in one single entry.

What about the observations in which player B won against player A? These will no longer be entries in the dataframe. However, this data will still be indirectly available by computing the total number of games between player A and player B (available in column $n_{games}$, at the corresponding row) minus the total number of victories of player A against player B (available in $n_{victories}$, at the corresponding row).

The final dataframe will look like as in Table (2).
\begin{table}
\begin{center}\begin{tabular}{cccc} $\textbf{Player1}$ & $\textbf{Player2}$ & $n_{games}$ & $n_{victories}$ \\Djockovic & Medvedev & 5 & 3 \\Djockovic & Nadal & 8 & 6 \\Nadal & Tsonga & 4 & 0\end{tabular} \caption{Final dataset}
\end{center}
\label{Final dataset}
\end{table}

\clearpage

\section{Descriptive statistics comparing simulated vs ATP data}


We start by choosing a particular configuration of parameters to simulate a given tournament to compare it with the ATP data.
\begin{itemize}
\item \texttt{N}=537 since in the ATP dataset we have exactly 537 players
\item \texttt{M}=3389 since in the ATP dataset this is the total number of matches
\item \texttt{max\_number\_games} = 4 since this is the max value we observe in the ATP data.
\end{itemize}
So, for these three initial parameters, we are replicating the features of the real tennis data. With respect to the more "statistical" ones there is no benchmark, so we choose to follow the intuition:
\begin{itemize}
\item \texttt{max\_clust}=3
\item \texttt{min\_clust}=3
\item \texttt{c} = 1.7
\item \texttt{beta\_max} = .8
\end{itemize}
After generating the data, we compare the simulated tournament and the true ATP data by computing some summary statistics and some meaningful plots shown in Figures (\ref{fig:scatterplot}), (\ref{fig:boxplotnij}), (\ref{fig:densityplotnij}), (\ref{fig:boxplotyij}), (\ref{fig:densityplotyij}). The scatterplot shows a similar pattern of strong correlation between $y_{ij}$ and $n_{ij}$. However, in ATP there is a higher points density close to the origin, while in the simulated tournament we have more density close to the mean. It means that real data are more dispersed than simulated ones. We can reach the same conclusion by looking at the number of games distribution and the number of victories. In particular, by looking at Figures (\ref{fig:boxplotyij}) and (\ref{fig:densityplotyij}) the discrepancy seems more enhanced for the number of victories $y_{ij}$. 

The interesting aspect of this methodology is that we could compute the discrepancy between the two distributions, for example using a Kullback-Leibler divergence, and find those parameters that minimize it.

\begin{figure}
\begin{minipage}{.8\textwidth}
      \centering
\includegraphics[width=.5\textwidth,natwidth=400,natheight=330]{/Users/lapo_santi/Desktop/Nial/project/descriptive statistic/comparison statistics/Scatterplot.png}
\caption{Scatterplot of $n_{ij}$, the number of games played between any two pair of players, and $y_{ij}$, the number of victories of player $i$ vs player $j$. We show the values both for Simulated (blue) and ATP data (red)}
\label{fig:scatterplot}
\end{minipage}\hfill
    \centering
    \begin{minipage}{0.25\textwidth}
        \centering
\includegraphics[width=\textwidth,natwidth=400,natheight=330]{/Users/lapo_santi/Desktop/Nial/project/descriptive statistic/comparison statistics/boxplotnij.png}
        \caption{Boxplot of $n_{ij}$ values both for Simulated (blue) and ATP data (red)}
        \label{fig:boxplotnij}
    \end{minipage}
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth,natwidth=400,natheight=330]{/Users/lapo_santi/Desktop/Nial/project/descriptive statistic/comparison statistics/densityplotnij.png}
        \caption{Density Plot for the $n_{ij}$ values both for Simulated (blue) and ATP data (red)}
        \label{fig:densityplotnij}
    \end{minipage}
    \vskip\floatsep
    \begin{minipage}{0.25\textwidth}
        \centering
 \includegraphics[width=\textwidth,natwidth=400,natheight=330]{/Users/lapo_santi/Desktop/Nial/project/descriptive statistic/comparison statistics/boxplotyij.png}
        \caption{Boxplot of $y_{ij}$ values both for Simulated (blue) and ATP data (red)}
        \label{fig:boxplotyij}
    \end{minipage}
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth,natwidth=400,natheight=330]{/Users/lapo_santi/Desktop/Nial/project/descriptive statistic/comparison statistics/densityplotyij.png}
        \caption{Density Plot for the $n_{ij}$ values both for Simulated (blue) and ATP data (red)}
        \label{fig:densityplotyij}
    \end{minipage}
\end{figure}



\end{comment}

\newpage 

\section{Estimation}

For the moment, we want to infer just $\theta = \{ z, P, \alpha, S \}$, meaning that we treat $K$ as a known constant. The estimation strategy is a Hybrid MCMC algorithm. Since simulating from the conditional distribution $p(\theta_i| \theta_j, j\neq i)$ is unfeasible or computationally expensive, we substitute the simulation from the full conditional distribution with a simulation from a proposal distribution $q_i$. Referencing Muller's (1991) work, the Hybrid modification is as follows:

\begin{algorithm}
\begin{algorithmic}[h]
\For{$i = 1,\ldots, p \quad \texttt{given} \quad \left( \theta_1^{(t+1)}, \dots,\theta_{i-1}^{(t+1)},\theta_i^{(t)}, \dots,\theta_{p}^{(t)}\right)  $}
\State \texttt{1. Simulate} \begin{equation}\label{eqn_general_proposal}
\theta_i^{\prime} \sim q_i \left(\theta_i^{(t)} | \theta_1^{(t+1)}, \dots,\theta_{i}^{(t)},\theta_{i+1}^{(t)}, \dots,\theta_{p}^{(t)} \right)
\end{equation}
\State \texttt{2. Take} \begin{equation}\label{eqn_acc_reject}
\theta_i^{(t+1)} = 
\begin{cases}
\theta_{i}^{(t)} \quad &\texttt{with probability} \quad 1 - r_i, \\
\theta_{i}^{\prime} \quad &\texttt{with probability} \quad 1 - r_i,
\end{cases}
\end{equation}
\State \texttt{where} \begin{equation}\label{eqn_gen_ratio}
r_i = 1 \vertg \left\{
\frac{\left(p(\theta_i^{\prime}|\theta_i^{(t)} | \theta_1^{(t+1)}, \dots,\theta_{i}^{(t)},\theta_{i+1}^{(t)}, \dots,\theta_{p}^{(t)}\right)}{\left(p(\theta_i^{(t)}|\theta_i^{(t)} | \theta_1^{(t+1)}, \dots,\theta_{i}^{(t)},\theta_{i+1}^{(t)}, \dots,\theta_{p}^{(t)}\right)}\right\}
\end{equation}
\EndFor
\end{algorithmic}
\label{alg_general_algortm}
\caption{Metropolis-within-Gibbs MCMC}
\end{algorithm}


\subsubsection{Adaptive algorithm for $\theta = \{ P, \alpha, S \}$}

We specify the proposal distributions in \eqref{eqn_general_proposal} above as $$\theta_i^{\prime} \sim \operatorname{Normal}\left( \theta_i^{(t-1)}, \sigma_{\theta_i}^2 \right)$$ whose sampled value is accepted or rejected by evaluating the logarithm of \eqref{eqn_gen_ratio}. Choosing a correct $\sigma_{\theta_i}^2 $ value is not straightforward, and we choose to resort to an adaptive algorithm to elicitate a correct proposal variance. We proceed as in Roberts, Rosenthal 2012. For each of the $K(K-1)/2 + 2$ parameters \(i\) \((1 \leq i \leq K(K-1)/2 + 2)\), we create an associated variable \(ls_i\) giving the logarithm of the standard deviation to be used when proposing a normal increment to variable \(i\). We begin with \(ls_i = \log{(0.04)}\) for all \(i\) (corresponding to 0.2 proposal standard deviation). After the \(n\)-th "batch" of 50 iterations, we update each \(ls_i\) by adding or subtracting an adaption amount \(\delta(n)\). The adapting attempts to make the acceptance rate of proposals for variable \(i\) as close as possible to 0.234, following the literature practice Chris Sherlock12009. Specifically, we increase \(ls_i\) by \(\delta(n)\) if the fraction of acceptances of variable \(i\) was more than 0.234 on the \(n\)-th batch, or decrease \(ls_i\) by \(\delta(n)\) if it was less.

$\rightarrow$ Insert here plots of convergence to the acceptance ratio

We specify in the Appendix the full expression for the ratio of $\theta = \{ P, \alpha, S \}$ in \eqref{eqn_gen_ratio}.

\subsubsection{Adaptive Algorithm for $\theta = { z }$}

When dealing with $\theta = { z }$, a discrete parameter, we need to adapt the formulation while maintaining the underlying concept. In the case of the POMM model, the labels $k = 1, \ldots, K$ are ordered, and therefore, we can define a distance metric between these labels. Let us denote the distance between $k$ and $k'$ as $d(k, k')$, which can be expressed as:

\begin{equation}
d(k, k^\prime) = |k - k^\prime|
\end{equation}

If the acceptance rate for a particular player $i$ is too low, we want the proposal to explore neighboring labels. Conversely, if the acceptance rate is too high, we aim to sample labels further away. To achieve this, we assign a sampling probability to each label that is inversely related to its distance from the current label. Specifically, we define $p(k') = p(|k' - k|) = \operatorname{Normal}(0, \sigma_i^2)$, where $\sigma_i^2$ is adapted as above. A larger variance assigns higher probabilities to distant labels, while a smaller variance favors closer labels. Finally, we employ a multinomial distribution to sample the next label $k'$:

\begin{equation}
k' \sim \operatorname{Multinomial}(1, K, p(|k' - k|))
\end{equation}

By utilizing this approach, we can adapt the algorithm to explore labels based on their distances from the current label. 

We specify in the Appendix the full expression for the ratio of $\theta = \{z\}$ in \eqref{eqn_gen_ratio}.


\clearpage

\section{Point Estimate, Model Selection,  and inference}

While algorithmic methods produce a single estimated partition, our model offers the entire posterior distribution across different node partitions. We are comparing the results from the simulation study via the following three main measures:

\begin{itemize}
\item Variation of Inforamation (VI): to fully utilise this posterior and engage in inference directly within the partition space, we adopt the decision-theoretic approach introduced by Wade and Ghahramani (2018) for block modeling. This involves summarizing posterior distributions using the variation of information (vi) metric, developed by Meilă (2007), which measures the distance between two clusterings by comparing their individual and joint entropies. The vi metric ranges from 0 to log2 V, where V represents the number of nodes. Intuitively, the vi metric quantifies the amount of information contained in two clusterings relative to the shared information between them. As a result, it decreases towards 0 as the overlap between two partitions increases. Refer to Wade and Ghahramani (2018) for a detailed exploration of the key properties of the vi metric. Within this framework, a formal Bayesian point estimate for z is obtained by selecting the partition with the lowest averaged vi distance from the other clusterings
\item WAIC: While the WAIC yields practical and theoretical advantages  and has direct connections with Bayesian leave-one-out cross-validation, thus providing a measure of edge predictive accuracy, the calculation of the WAIC only requires posterior samples of the log-likelihoods for the edges:$
\log p(y_{ij} | z, P, \alpha) = y_{ij} \log p_{z_i, z_j} + (n_{ij}- y_{ij}) \log(1 - p_{z_i, z_j}), \quad i = 2, \ldots, N,  j = 1, \ldots, i - 1$.

\item Misclassification error: predicting the group membership $z_{N+1}$ of a new player may also be of interest. We can derive the estimate of the block probabilities for new players based on their early matches with some of the existing players.
\begin{align}
p(z_{N+1} = k | \textbf{Y}, y_{N+1}, \hat{z}) &\propto p(y_{N+1} | \textbf{Y}, \hat{z}, z_{N+1} = k ) \cdot p(z_{N+1} = k | \hat{z}) \nonumber \\
&= p(y_{N+1} | \hat{z}, z_{N+1} = k ) \cdot p(z_{N+1} = k | \hat{z})
\end{align}
where $p(z_{N+1} = k | \textbf{Y}, y_{N+1}, \hat{z})$ is the posterior probability of the new node $N+1$ to belong to the block $k$, given the previously observed data $Y$, the new node's data $y_{N+1}$ and the estimated labels $\hat{z}$. On the right hand side of the expression above, $p(y_{N+1} | \textbf{Y}, \hat{z}, z_{N+1} = k )$ represents the likelihood of observing $y_{N+1}$ given the previously observed data $Y$ and the estimated labels $\hat{z}$, which, due to conditional independence, is the same as conditioning just on $\hat{z}$. Finally, $p(z_{N+1} = k | \hat{z})$ represents the prior probability of label $k$ for the new node $N+1$ given $\hat{z}$, which we can approximate with the relative size of the blocks $n_k$.
\end{itemize}
\begin{comment}
\item Prediction error: making prediction on the match score between two players is also possible within this framework. Once we have obtained the MCMC samples, we can retrieve the posterior predictive distribution.

Let us assume $i^\star$ is a new player . The posterior predictive is:
\begin{align}
p(y_{i^\star j}| \textbf{Y}) &= \int p(z| \{ \textbf{Y}\setminus y_{i^\star} \})  \cdot p(y_{i^\star j} | z ) dz \\
&\approx \frac{1}{T} \sum_{t = 1}^{T} \sum_{z_i=1}^{K}  p(y_{i^\star j}| z_j^{(t)}) \\
&=  \frac{1}{T} \sum_{t = 1}^{T} \sum_{z_{i^\star} =1}^{K}  p_{z_{i^\star}, z_j^{(t)}}^{y_{i^\star j} } \cdot (1-p_{z_{i^\star}, z_j^{(t)}})^{n_{i^\star j} -  y_{i^\star j}} \quad \quad \text{for }y_{i^\star j} = 0,\ldots, n_{i^\star j} \\
\end{align} 
where $z_j^{(t)} \sim p(z|Y)$
\end{itemize}
\end{comment}

\clearpage

\section{Simulation Study from the Simple Model}

In order to evaluate how well our model performs in a situation similar to our intended use, and measure its advantages compared to the best existing alternatives, we generated three simulated tournaments with 100 players from the Simple Model. We want to compare how it performs compared to the POMM extension and other state-of-the-art alternatives. Each tournament had a different number of blocks in the underlying structure. We set the total number of games $M := 0.5* \sum_{i,j}^N n_{ij}= \sum_{i,j}^N y_{ij} = 4000$, which is the average number of matches played in one year of tennis tournaments. We divided the players into three, five and nine blocks ($K = 3,5,9$ respectively). In Figure (\ref{fig:simple_adjacency}), we display the three simulated tournaments, where the difficulty of accurately determining the group membership increases as the number of games increases with the number of blocks.

\begin{figure}[htbp]
    \centering
    \subfigure[K=3, Simple Model]{%
        \includegraphics[width=.33\textwidth,natwidth=476,natheight=481]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/results_plots/simple_adjacency_K3.png}%
        \label{fig:simple_adjacency_K3}%
    }\hfill
    \subfigure[K=5, Simple Model]{%
        \includegraphics[width=.33\textwidth,natwidth=476,natheight=481]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/results_plots/simple_adjacency_K5.png}%
        \label{fig:simple_adjacency_K5}%
    }\hfill
    \subfigure[K=9, Simple Model]{%
        \includegraphics[width=.33\textwidth,natwidth=476,natheight=481]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/results_plots/simple_adjacency_K9.png}%
        \label{fig:simple_adjacency_K9}%
    }
    \caption{Adjacency Matrices simulated via the Simple Model}
    \label{fig:simple_adjacency}
\end{figure}

We compare the performance of the Simple model with the POMM one. We fixed arbitrarily $\beta_{\max} = .75$.
In table (\ref{table:simulations_from_simple}) we report the results of the simulation. In the three cases, for the Simple and the POMM model, we compare the WAIC, the VI distance and the misclassification error, obtained by considering 100 new incoming players which get to play just with 10 players each. We also compare the labels estimated against the regularised spectral clustering algorithm and the Louvain algorithm. The Simple model is the best performing relative to the other three alternatives.

\begin{figure}[htbp]
    \centering
    \subfigure[K=3, Simple Model]{%
        \includegraphics[width=.3333\textwidth,natwidth=474,natheight=479]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/results_plots/SimpleSimple_similarity_K3.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=5, Simple Model]{%
        \includegraphics[width=.3333\textwidth,natwidth=474,natheight=479]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/results_plots/SimpleSimple_similarity_K5.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=9, Simple Model]{%
        \includegraphics[width=.3333\textwidth,natwidth=474,natheight=479]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/results_plots/SimpleSimple_similarity_K9.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[K=3, POMM Model]{%
        \includegraphics[width=.3333\textwidth,natwidth=474,natheight=479]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/results_plots/SimplePOMM_similarity_K3.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=5, POMM Model]{%
        \includegraphics[width=.3333\textwidth,natwidth=474,natheight=479]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/results_plots/SimplePOMM_similarity_K5.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=9, POMM Model]{%
        \includegraphics[width=.3333\textwidth,natwidth=474,natheight=479]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/results_plots/SimplePOMM_similarity_K9.png}%
        \label{fig:M10000}%
    }
    \caption{Co-Clustering Matrices obtained via the Simple Model(above) and the POMM model (below).}
    \label{fig:adjacency_all_simple}
\end{figure}


In figure (\ref{fig:adjacency_all_simple}) we report the estimated co-clustering matrices resulting from the simulation process.






\begin{table}[htbp]
\centering
\caption{$Y_{ij}$ drawn from a simple model}
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{WAIC} & \multicolumn{3}{c}{VI distance} & \multicolumn{3}{c}{Error} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
& $(a)$ & $(b)$ & $(c)$ & $(a)$ & $(b)$ & $(c)$ & $(a)$ & $(b)$ & $(c)$ \\
\midrule
Simple model &  $\underset{[44.5]}{-12040.4}$ & $\underset{[26.5]}{-10483.7}$ & $\underset{[35.4]}{-11514.6}$ & 0 & 2.70 & 1.84 & 0.66 & 0.71 & 0.91 \\
POMM model & $\underset{[39.6]}{-11076.5}$ & $\underset{[29.8]}{-10500.8}$& $\underset{[33.2]}{-11173.7}$ & 0.12 & 2.59 & 2.58 & 0.6 & 0.73 & 0.88  \\
Spectral Clustering & - & - & - & 0.26 & 4.06 & 4.55 & - & - & -  \\
Louvain algorithm & - & - & - & 4.90 & 3.43 & 4.12 & - & - & -  \\
\bottomrule
\end{tabular}
\label{table:simulations_from_simple}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Data Table}
  \label{tab:data}
  \begin{tabular}{ccc}
    \toprule
    Configuration & $\hat{\alpha}$ (95\% CI) & $\hat{S}$ (95\% CI) \\
    \midrule
    K3\_M4000 & $0.43 \underset{[0.14, 0.83]}{}$ & $0.53 \underset{[0.27, 0.84]}{}$ \\
    K5\_M4000 & $0.83 \underset{[0.74, 0.90]}{}$ & $0.85 \underset{[0.76, 0.90]}{}$ \\
    K9\_M4000 & $0.82 \underset{[0.72, 0.89]}{}$ & $0.88 \underset{[0.84, 0.90]}{}$ \\
    \bottomrule
  \end{tabular}
\end{table}




\begin{table}[htbp]
\centering
\caption{$Y_{ij}$ drawn from a simple model}
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{
\% $\{p \in \text{CI}_{95\%}\}$} & \multicolumn{3}{c}{
\% $\{p \in \text{CI}_{99\%}\}$} & \multicolumn{3}{c}{mean MSE} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
& $(a)$ & $(b)$ & $(c)$ & $(a)$ & $(b)$ & $(c)$ & $(a)$ & $(b)$ & $(c)$ \\
\midrule
Simple model & 1 &  0.9 & 0.61 & 1&1 & 0.71 & 0.01 & 0.08 & 0.11 \\
POMM model  &0.33&  0.5 & 0.25 & 0.33 & 0.5  & 0.31 & 0.12 & 0.09 & 0.10  \\
\bottomrule
\end{tabular}
\label{table:simulations_from_simple}
\end{table}
\clearpage
\section{Simulation Study from the POMM Model}

In this section we reverse the exercise performed in previous one. Before we were simulating from the Simple model, now we are doing the same, with similar parameters ($K=3,5,9, M=4000 \text{ and } \beta_{\max} = .75)$. Here are the results.

\begin{figure}[h]
    \centering
    \subfigure[K=3, POMM Model]{%
        \includegraphics[width=.33\textwidth,natwidth=472,natheight=478]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/results_plots/POMM_adjacency_K3.png}%
        \label{fig:POMM_adjacency_K3}%
    }\hfill
    \subfigure[K=5, POMM Model]{%
        \includegraphics[width=.33\textwidth,natwidth=472,natheight=478]{//Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/results_plots/POMM_adjacency_K5.png}%
        \label{fig:POMM_adjacency_K5}%
    }\hfill
    \subfigure[K=9, POMM Model]{%
        \includegraphics[width=.33\textwidth,natwidth=472,natheight=478]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/results_plots/POMM_adjacency_K9.png}%
        \label{fig:POMM_adjacency_K9}%
    }
    \caption{Adjacency Matrices simulated via the POMM Model}
    \label{fig:all_images}
\end{figure}

\begin{figure}[htbp]
    \centering
    \subfigure[K=3, Simple Model Estimates]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/old_studies/plots/similarity_SimpleK3_M4000.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=5, Simple Model Estimates]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/old_studies/plots/similarity_SimpleK5_M4000.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=9, Simple Model Estimates]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/old_studies/plots/similarity_SimpleK9_M4000.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[K=3, POMM Model Estimates]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/old_studies/plots/similarity_POMMK3_M4000.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=5, POMM Model Estimates]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/old_studies/plots/similarity_POMMK5_M4000.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=9, POMM Model Estimates]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/old_studies/plots/similarity_POMMK9_M4000.png}%
        \label{fig:M10000}%
    }
    \caption{Co-Clustering Matrices obtained via the Simple Model(above) and the POMM model (below).}
    \label{fig:all_images}
\end{figure}



In table (\ref{table:simulations_from_simple}) we report the results of the simulation. As before, for the Simple and the POMM model, we compare the WAIC, the VI distance and the misclassification error, obtained $N_new = 100$. Also here we compare clustering performance against that of the regularised spectral clustering algorithm and the Louvain algorithm. The POMM model is the best performing relative to the other three alternatives.




\begin{table}[htbp]
\centering
\caption{$Y_{ij}$ drawn from the POMM model}
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{WAIC} & \multicolumn{3}{c}{VI distance} & \multicolumn{3}{c}{Error} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
& $(a)$ & $(b)$ & $(c)$ & $(a)$ & $(b)$ & $(c)$ & $(a)$ & $(b)$ & $(c)$ \\
\midrule
Simple model &  $\underset{[35.31]}{-11027.45}$ & $\underset{[33.51]}{-10991.22}$ & $\underset{[34.05]}{-11621.91}$ & 0.79 & 2.88 & 4.11 & 0.73 & 0.70 & 0.90 \\
POMM model & $\underset{[35.42]}{-11030.84}$ & $\underset{[33.47]}{-10963.05}$& $\underset{[33.49]}{-11409.65}$ & 0.79 & 2.81 & 3.30 & 0.60 & 0.75 & 0.93\\
Spectral Clustering & - & - & - & 0.95 & 1.85 & - & - & - & - \\
Louvain algorithm & - & - & - & 3.94 & 4.26 & - & - & - & - \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}[htbp]
  \centering
  \caption{Data Table, $\alpha=0.5$, $S=0.2$}
  \label{tab:data}
  \begin{tabular}{ccc}
    \toprule
    Configuration & $\hat{\alpha}$ (95\% CI) & $\hat{S}$ (95\% CI)  \\
    \midrule
    K3\_M4000 & $0.4442 \underset{[0.1930, 0.7855]}{}$ & $0.2845 \underset{[0.1122, 0.6842]}{}$ \\
    K5\_M4000 & $0.7357 \underset{[0.5367, 0.8844]}{}$ & $0.7498 \underset{[0.5573, 0.8877]}{}$ \\
    K9\_M4000 & $0.7696 \underset{[0.6254, 0.8865]}{}$ & $0.8707 \underset{[0.8247, 0.8981]}{}$ \\
    \bottomrule
  \end{tabular}
\end{table}





\begin{table}[htbp]
\centering
\caption{$Y_{ij}$ drawn from a simple model}
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{
\% $\{p \in \text{CI}_{95\%}\}$} & \multicolumn{3}{c}{
\% $\{p \in \text{CI}_{99\%}\}$} & \multicolumn{3}{c}{mean MSE} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
& $(a)$ & $(b)$ & $(c)$ & $(a)$ & $(b)$ & $(c)$ & $(a)$ & $(b)$ & $(c)$ \\
\midrule
Simple model & 0.67 &  0.8 & 0.78 & 1&0.80 & 0.97 & 0.01 & 0.05 & 0.15 \\
POMM model  &0.67&  0.7 & 0.97 & 0.67 & 0.80  & 1 & 0.01 & 0.04 & 0.09  \\
\bottomrule
\end{tabular}
\label{table:simulations_from_simple}
\end{table}

\clearpage


\subsection{Extended Simulation Study}
In this section, we want to investigate the performance of the POMM model in recovering the true known partition, generated via the POMM model itself by changing the number of clusters involved ($K$) and the number of games played among the players $M$.

\begin{table}[htbp]
  \centering
  \caption{Data Table K=3}
  \label{tab:data}
  \begin{tabular}{ccccc}
    \toprule
    CONFIGURATION& MAP & MINVI & MISCLASSERROR & MSE\_sum \\
    \midrule
    $M=4000$ & 1.1246508 & 0.9617248 & 0.6333333 & 0.054827731 \\
    $M=10000$ & 0.2557382 & 0.2557382 & 0.6166667 & 0.007170147 \\
    $M=40000$ & 0.0000000 & 0.0000000 & 0.6166667 & 0.003346822 \\
    \bottomrule
  \end{tabular}
\end{table}
\begin{table}[htbp]
  \centering
  \caption{Data Table K=5}
  \label{tab:data}
  \begin{tabular}{ccccc}
    \toprule
    CONFIGURATION& MAP & MINVI & MISCLASSERROR & MSE\_sum \\
    \midrule
    $M=4000$ & 2.8488118 & 2.7704420 & 0.7333333 & 0.095988871 \\
    $M=10000$ & 1.6784835 & 1.6024773 & 0.8000000 & 0.059037933 \\
    $M=40000$ & 0.4500958 & 0.4500958 & 0.6666667 & 0.057649622 \\
    \bottomrule
  \end{tabular}
\end{table}
\begin{table}[htbp]
  \centering
  \caption{Data Table K=9}
  \label{tab:data}
  \begin{tabular}{ccccc}
    \toprule
    CONFIGURATION& MAP & MINVI & MISCLASSERROR & MSE\_sum \\
    \midrule
    $M=4000$ & 3.6752208 & 3.7966766 & 0.8166667 & 0.051228350 \\
    $M=10000$ & 2.8036489 & 2.6477037 & 0.8166667 & 0.047891014 \\
    $M=40000$ & 1.7268339 & 1.6557098 & 0.9000000 & 0.033409231 \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{table}[htbp]
  \centering
  \caption{Data Table, $K =3, \alpha=0.5$, $S = 0.2$}
  \label{tab:data}
  \begin{tabular}{ccc}
    \toprule
    Configuration & $\hat{\alpha}$ (95\% CI) & $\hat{S}$ (95\% CI) \\
    \midrule
        M=4000 & $0.6275 \underset{[0.3515, 0.8679]}{}$ & $0.5018 \underset{[0.2251, 0.8398]}{}$ \\
    M=10000 & $0.5054 \underset{[0.2600, 0.8057]}{}$ & $0.3098 \underset{[0.1263, 0.6942]}{}$ \\
    M=40000 & $0.4802 \underset{[0.2452, 0.7880]}{}$ & $0.3308 \underset{[0.1343, 0.7215]}{}$ \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Data Table, $K =5, \alpha=0.5$, $S = 0.2$}
  \label{tab:data}
  \begin{tabular}{ccc}
    \toprule
    Configuration & $\hat{\alpha}$ (95\% CI) & $\hat{S}$ (95\% CI) \\
    \midrule
        M=4000 & $0.7497 \underset{[0.5944, 0.8841]}{}$ & $0.8260 \underset{[0.7178, 0.8950]}{}$ \\
    M=10000 & $0.6905 \underset{[0.4872, 0.8734]}{}$ & $0.6801 \underset{[0.4702, 0.8735]}{}$ \\
    M=40000 & $0.7888 \underset{[0.6365, 0.8918]}{}$ & $0.7136 \underset{[0.5171, 0.8816]}{}$ \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Data Table, $K =9, \alpha=0.5$, $S = 0.2$}
  \label{tab:data}
  \begin{tabular}{ccc}
    \toprule
    Configuration & $\hat{\alpha}$ (95\% CI) & $\hat{S}$ (95\% CI) \\
    \midrule
     M = 4000 & $0.5966 \underset{[0.5966, 0.5966]}{}$ & $0.8699 \underset{[0.8215, 0.8980]}{}$ \\
       M=10000 & $0.6038 \underset{[0.5989, 0.6378]}{}$ & $0.8574 \underset{[0.7899, 0.8971]}{}$ \\
    M = 40000 & $0.7241 \underset{[0.6251, 0.8412]}{}$ & $0.8330 \underset{[0.7290, 0.8959]}{}$ \\
    \bottomrule
  \end{tabular}
\end{table}







\clearpage
\begin{figure}[h]
    \centering
    \subfigure[K=3, M=4000, Adjacency ]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/adjacency_POMM_K3_M4000.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=3, M=4000, Adjacency ]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/adjacency_POMM_K3_M10000.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=3, M=10000, Adjacency ]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/adjacency_POMM_K3_M40000.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[K=3, M=40000, Similarity ]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/similarity_POMM_K3_M4000.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=3, M=4000, Similarity ]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/similarity_POMM_K3_M10000.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=3, M=10000, Similarity ]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/similarity_POMM_K3_M40000.png}%
        \label{fig:M40000}%
    }
    \caption{Co-Clustering Matrices obtained via the Simple Model(above) and the POMM model (below).}
    \label{fig:all_images}
\end{figure}
\clearpage
\begin{figure}[h]
    \centering
    \subfigure[K=5, M=4000 Adjacency]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/adjacency_POMM_K5_M4000.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=5, M=10000 Adjacency]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/adjacency_POMM_K5_M10000.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=5, M=40000 Adjacency]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/adjacency_POMM_K5_M40000.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[K=5, M=4000 Similarity]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/similarity_POMM_K5_M4000.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=5, M=10000 Similarity]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/similarity_POMM_K5_M10000.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=5, M=40000 Similarity]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/similarity_POMM_K5_M40000.png}%
        \label{fig:M10000}%
    }
    \caption{Co-Clustering Matrices obtained via the Simple Model(above) and the POMM model (below).}
    \label{fig:all_images}
\end{figure}
\clearpage
\begin{figure}[h]
    \centering
    \subfigure[K=9, M=4000 Adjacency]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/adjacency_POMM_K9_M4000.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=9, M=10000 Adjacency]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/adjacency_POMM_K9_M10000.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=9, M=40000 Adjacency]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/adjacency_POMM_K9_M40000.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[K=9, M=4000 Similarity]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/similarity_POMM_K9_M4000.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=9, M=10000 Similarity]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/similarity_POMM_K9_M10000.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=9, M=40000 Similarity]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=800]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/similarity_POMM_K9_M40000.png}%
        \label{fig:M10000}%
    }
    \caption{Co-Clustering Matrices obtained via the Simple Model(above) and the POMM model (below).}
    \label{fig:all_images}
\end{figure}



\subsection{Diagnostic Checks}


\begin{figure}[h]
    \centering
    \subfigure[K=3, M=4000 Traceplot]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/traceplot_POMM_K3_M4000.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=3, M=10000 Traceplot]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/traceplot_POMM_K3_M10000.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=3, M=40000 Traceplot]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/traceplot_POMM_K3_M40000.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[K=3, M=4000 Autocorrelation]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/autocorrplot_POMM_K3_M4000.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=3, M=10000 Autocorrelation]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/autocorrplot_POMM_K3_M10000.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=3, M=40000 Autocorrelation]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/autocorrplot_POMM_K3_M40000.png}%
        \label{fig:M10000}%
    }
    \caption{Co-Clustering Matrices obtained via the Simple Model(above) and the POMM model (below).}
    \label{fig:all_images}
\end{figure}

\clearpage

\begin{figure}[h]
    \centering
    \subfigure[K=5, M=4000 Traceplot]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/traceplot_POMM_K5_M4000.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=5, M=10000 Traceplot]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/traceplot_POMM_K5_M10000.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=5, M=40000 Traceplot]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/traceplot_POMM_K5_M40000.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[K=5, M=4000 Autocorrelation]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/autocorrplot_POMM_K5_M4000.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=5, M=10000 Autocorrelation]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/autocorrplot_POMM_K5_M10000.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=5, M=40000 Autocorrelation]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/autocorrplot_POMM_K5_M40000.png}%
        \label{fig:M10000}%
    }
    \caption{Co-Clustering Matrices obtained via the Simple Model(above) and the POMM model (below).}
    \label{fig:all_images}
\end{figure}
\clearpage
\begin{figure}[h]
    \centering
    \subfigure[K=9, M=4000 Traceplot]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/traceplot_POMM_K9_M4000.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=9, M=10000 Traceplot]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/traceplot_POMM_K9_M10000.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=9, M=40000 Traceplot]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/traceplot_POMM_K9_M40000.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[K=9, M=4000 Autocorrelation]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/autocorrplot_POMM_K9_M4000.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[K=9, M=10000 Autocorrelation]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/autocorrplot_POMM_K9_M10000.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[K=9, M=40000 Autocorrelation]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=518]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/MCMC/results/differentKstudy/autocorrplot_POMM_K9_M40000.png}%
        \label{fig:M10000}%
    }
    \caption{Co-Clustering Matrices obtained via the Simple Model(above) and the POMM model (below).}
    \label{fig:all_images}
\end{figure}

\clearpage
\section{Application to Tennis Data}


\clearpage

\section{Appendix I: Investigating Empirically the prior behaviour}


In this section, we explore the behaviour of the POMM prior, as $\alpha$, the parameter controlling the rate of increase of the power-law process and $S$, the variation (and therefore, the overlap) of the level sets sets, change. 

We start with a simulation study, whose results are reported in figure \eqref{fig}.

\begin{figure}[h]
    \centering
       \subfigure[$\alpha=0.1, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha0.1overlap0.2K5.png}%
        \label{l}%
    }\hfill
    \subfigure[$\alpha=0.1, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha0.1overlap0.4K5.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=0.1, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha0.1overlap0.7K5.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[$\alpha=0.5, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha0.5overlap0.2K5.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[$\alpha=0.5, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha0.5overlap0.4K5.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=0.5, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha0.5overlap0.7K5.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[$\alpha=1, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha1overlap0.2K5.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[$\alpha=1, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha1overlap0.4K5.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=1, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha1overlap0.7K5.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[$\alpha=1.5, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha1.5overlap0.2K5.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[$\alpha=1.5, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha1.5overlap0.4K5.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=1.5, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha1.5overlap0.7K5.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[$\alpha=3, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha3overlap0.2K5.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[$\alpha=3, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha3overlap0.4K5.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=3, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/alpha_and_overlap_investigation_alpha3overlap0.7K5.png}%
        \label{fig:M10000}%
    }
    \caption{Distribution of $n=1000$ simulated $P$ matrices with $\alpha \in \{0.1,0.5,1,1.5,3\}$ and $S \in \{0.2,0.4,0.7\}$. The points are grouped into the $K=5$ level sets- the main diagonal is set =0.5. The black vertical dots are the points, while the red lines show the evolution of the mean of the level sets.}
    \label{fig:exploratoryanalysisalpha_andS}
\end{figure}


\clearpage


\section{Appendix II: Empirical Assessment of the inference problem}

This section has two main objectives:

\begin{enumerate}
\item The first objective is to assess the identifiability of the POMM model. Given that the richness of the model, it is not obvious that the parameters are identifiable in all the regions of the parameter space. Therefore, by means of several simulation studies, we aim at identifying any possible identifiability constraints.
\item The second objective is code testing, by checking the whole estimation procedure on simulated data.
\end{enumerate}

\subsection{Focus on $\alpha$}
\begin{longtable}{l|rrr}
\caption{
{\large Mean absolute error for different alpha values} \\ 
{\small Sample size = 1000. MAEs = $|\hat{\alpha}_{\text{MLE}} - \alpha|$}
} \\ 
\toprule
\multicolumn{1}{l}{} & S = 0.2 & S = 0.4 & S = 0.7 \\ 
\midrule
$\alpha=$ 0.1 & $3.199 \times 10^{-4}$ & $1.270 \times 10^{-4}$ & $0.003$ \\ 
$\alpha=$ 0.5 & $1.377 \times 10^{-4}$ & $2.955 \times 10^{-4}$ & $0.001$ \\ 
$\alpha=$ 1.0 & $7.695 \times 10^{-4}$ & $0.007$ & $0.001$ \\ 
$\alpha=$ 1.5 & $0.001$ & $0.006$ & $0.004$ \\ 
$\alpha=$ 3.0 & $0.003$ & $0.036$ & $0.004$ \\ 
\bottomrule
\end{longtable}



\begin{figure}[h]
    \centering
       \subfigure[$\alpha=0.1, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha0.1_overlap0.2_diag_.png}%
        \label{l}%
    }\hfill
    \subfigure[$\alpha=0.1, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha0.1_overlap0.4_diag_.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=0.1, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha0.1_overlap0.7_diag_.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[$\alpha=0.5, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha0.5_overlap0.2_diag_.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[$\alpha=0.5, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha0.5_overlap0.4_diag_.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=0.5, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha0.5_overlap0.7_diag_.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[$\alpha=1, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha1_overlap0.2_diag_.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[$\alpha=1, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha1_overlap0.4_diag_.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=1, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha1_overlap0.7_diag_.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[$\alpha=1.5, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha1.5_overlap0.2_diag_.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[$\alpha=1.5, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha1.5_overlap0.4_diag_.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=1.5, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha1.5_overlap0.7_diag_.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[$\alpha=3, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha3_overlap0.2_diag_.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[$\alpha=3, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha3_overlap0.4_diag_.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=3, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/LikelihoodK5_alpha3_overlap0.7_diag_.png}%
        \label{fig:M10000}%
    }
    \caption{Distribution of $n=1000$ simulated $P$ matrices with $\alpha \in \{0.1,0.5,1,1.5,3\}$ and $S \in \{0.2,0.4,0.7\}$. The points are grouped into the $K=5$ level sets- the main diagonal is set =0.5. The black vertical dots are the points, while the red lines show the evolution of the mean of the level sets.}
    \label{fig:exploratoryanalysisalpha_andS}
\end{figure}


\subsubsection{Updating $\alpha$ algorithm}

3


\subsubsection{Montecarlo algorithm}



\subsection{Focus on $S$}

\begin{longtable}{l|rrrrr}
\caption*{
{\large Mean absolute error for different $S$ values} \\ 
{\small Each row is a different $S$ value. Sample size = 1000}
} \\ 
\toprule
\multicolumn{1}{l}{} & $\alpha = 0.1$ & $\alpha = 0.5$ & $\alpha = 1$ & $\alpha = 1.5$ & $\alpha = 3$ \\ 
\midrule
S = 0.2 & $0.002$ & $0.002$ & $9.780 \times 10^{-4}$ & $3.709 \times 10^{-4}$ & $9.931 \times 10^{-4}$ \\ 
S = 0.4 & $0.005$ & $0.003$ & $0.006$ & $0.002$ & $0.003$ \\ 
S = 0.7 & $0.006$ & $0.002$ & $0.005$ & $0.006$ & $0.007$ \\ 
\bottomrule
\end{longtable}


\begin{figure}[h]
    \centering
       \subfigure[$\alpha=0.1, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha0.1_overlap0.2_diag_.png}%
        \label{l}%
    }\hfill
    \subfigure[$\alpha=0.1, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha0.1_overlap0.4_diag_.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=0.1, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha0.1_overlap0.7_diag_.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[$\alpha=0.5, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha0.5_overlap0.2_diag_.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[$\alpha=0.5, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha0.5_overlap0.4_diag_.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=0.5, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha0.5_overlap0.7_diag_.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[$\alpha=1, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha1_overlap0.2_diag_.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[$\alpha=1, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha1_overlap0.4_diag_.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=1, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha1_overlap0.7_diag_.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[$\alpha=1.5, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha1.5_overlap0.2_diag_.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[$\alpha=1.5, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha1.5_overlap0.4_diag_.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=1.5, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha1.5_overlap0.7_diag_.png}%
        \label{fig:M10000}%
    }\\[2ex]\subfigure[$\alpha=3, S = 0.2$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha3_overlap0.2_diag_.png}%
        \label{fig:M4000}%
    }\hfill
    \subfigure[$\alpha=3, S = 0.4$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha3_overlap0.4_diag_.png}%
        \label{fig:M7000}%
    }\hfill
    \subfigure[$\alpha=3, S = 0.7$]{%
        \includegraphics[width=.3333\textwidth,natwidth=800,natheight=438]{/Users/lapo_santi/Desktop/Nial/POMM_pairwise/POMMs/POMM_flex/exploratory_graphs_with_inference/Overlap_LikelihoodK5_alpha3_overlap0.7_diag_.png}%
        \label{fig:M10000}%
    }
    \caption{Distribution of $n=1000$ simulated $P$ matrices with $\alpha \in \{0.1,0.5,1,1.5,3\}$ and $S \in \{0.2,0.4,0.7\}$. The points are grouped into the $K=5$ level sets- the main diagonal is set =0.5. The black vertical dots are the points, while the red lines show the evolution of the mean of the level sets.}
    \label{fig:exploratoryanalysisalpha_andS}
\end{figure}


\subsubsection{Montecarlo algorithm}








\clearpage

\section{Appendix I: Estimation Details}


\subsection{Updating z}

To update $z$ we propose a new label for each node, we evaluate the accept/reject move by computing the ratio $r$ as follows:
\begin{align}
r &= \frac{\prod_{i<j}\binom{n_{ij}}{y_{ij}}p_{z^{\prime}_i z^{\prime}_j}^{y_{ij}} \cdot (1 - p_{z^{\prime}_i z^{\prime}_j})^{n_{ij} - y_{ij}} \cdot \frac{\Gamma(\gamma_0) \Gamma(n+1)}{\Gamma(n + \gamma_0)} \cdot \prod_{k=1}^K \frac{\Gamma(n^{\prime}_k + \gamma_k)}{\Gamma(\gamma_k)  \Gamma(n^{\prime}_k + 1)}}{\prod_{i<j}\binom{n_{ij}}{y_{ij}}p_{z_iz_j}^{y_{ij}} \cdot (1 - p_{z_iz_j})^{n_{ij} - y_{ij}}\cdot \frac{\Gamma(\gamma_0) \Gamma(n+1)}{\Gamma(n + \gamma_0)} \cdot \prod_{k=1}^K \frac{\Gamma(n_k + \gamma_k)}{\Gamma(\gamma_k)  \Gamma(n_k + 1)}} \\
 &= \frac{\prod_{i<j}p_{z^{\prime}_i z^{\prime}_j}^{y_{ij}} \cdot (1 - p_{z^{\prime}_i z^{\prime}_j})^{n_{ij} - y_{ij}} \cdot  \prod_{k=1}^K \frac{\Gamma(n^{\prime}_k + \gamma_k)}{\Gamma(\gamma_k)  \Gamma(n^{\prime}_k + 1)}}{\prod_{i<j}p_{z_iz_j}^{y_{ij}} \cdot (1 - p_{z_iz_j})^{n_{ij} - y_{ij}} \cdot \prod_{k=1}^K \frac{\Gamma(n_k + \gamma_k)}{\Gamma(\gamma_k)  \Gamma(n_k + 1)}} 
\end{align}


Passing to the log:

\begin{align}
log(r) &= \log{ \left( \prod_{i<j}p_{z^{\prime}_i z^{\prime}_j}^{y_{ij}} \cdot (1 - p_{z^{\prime}_i z^{\prime}_j})^{n_{ij} - y_{ij}} \cdot  \prod_{k=1}^K \frac{\Gamma(n^{\prime}_k + \gamma_k)}{\Gamma(\gamma_k)  \Gamma(n^{\prime}_k + 1)} \right) }  \nonumber \\
& \qquad - \log{ \left( \prod_{i<j}p_{z_iz_j}^{y_{ij}} \cdot (1 - p_{z_iz_j})^{n_{ij} - y_{ij}} \cdot \prod_{k=1}^K \frac{\Gamma(n_k + \gamma_k)}{\Gamma(\gamma_k)  \Gamma(n_k + 1)}\right)} \nonumber \\
&= \sum_{i<j} \left(   y_{ij} \cdot \log{ p_{z^{\prime}_i z^{\prime}_j} } + (n_{ij} - y_{ij}) \cdot \log{ (1 - p_{z^{\prime}_i z^{\prime}_j}) } \right)\nonumber \\ 
&\qquad +  \sum_{k=1}^K\left(\log\left(\Gamma(n^{\prime}_{k}+\gamma_{k})\right) - \log\left(\Gamma(\gamma_{k})\right) - \log\left(\Gamma\left(n^{\prime}_{k}+1\right)\right) \right)  \nonumber  \\
& \qquad \qquad - \sum_{i<j} \left(  y_{ij} \cdot \log{ p_{z_i z_j} } + (n_{ij} - y_{ij}) \cdot \log{ (1 - p_{z_i z_j}) } \right) \nonumber \\
&\qquad \qquad \qquad - \sum_{k=1}^K\left(\log\left(\Gamma(n_{k}+\gamma_{k})\right) - \log\left(\Gamma(\gamma_{k})\right) - \log\left(\Gamma\left(n_{k}+1\right)\right) \right) \nonumber \\
\end{align}


\begin{algorithm}
\begin{algorithmic}[1]
\For{$i \gets 1$ to $N$}
\State Sample $\texttt{new\_label}$ from $1,...,K$
\State Set $z^{\prime} \gets z$ with the $i$-th element replaced by $\texttt{new\_label}$
\State Compute new victory probabilities $p_{z^{\prime}_i z^{\prime}_j}$ using $z^{\prime}$
\State Compute probability ratio $log(r)$  using $p_{z^{\prime}_i z^{\prime}_j}$ and $p_{z_i z_j}$
\State Set $\alpha_{r} \gets \min(1, r)$
\State Sample $u$ from a uniform distribution on $(0,1)$
\If{$u < \alpha_{r}$}
\State Update $z$ to $z^{\prime}$
\State Update $p_{z_iz_j}$ to $p_{z^{\prime}_i z^{\prime}_j}$
\State Increment $acc.count_{z}$
\EndIf
\State Store $z_{current}$ in $z.container$
\EndFor
\end{algorithmic}
\label{alg:z_update}
\caption{Updating $z$ step}
\end{algorithm}


\subsection{Updating $\mathbf{P}$}


To update $P$ and $\alpha$ we propose a new label for each node, we evaluate the accept/reject move by computing the ratio $r$ as follows:

\begin{align}
r &= \frac{\prod_{i<j}\binom{n_{ij}}{y_{ij}}p_{z_i z_j}^{\prime y_{ij}} \cdot (1 - p^{\prime}_{z_i z_j})^{n_{ij} - y_{ij}} \cdot \prod_{k=1}^K  \left( \frac{1}{y^{\prime (k+1)} - y^{\prime(k)}}\right)^{|L^{\prime(k)}|}}{\prod_{i<j}\binom{n_{ij}}{y_{ij}}p_{z_i z_j}^{y_{ij}} \cdot (1 - p_{z_i z_j})^{n_{ij} - y_{ij}} \cdot \prod_{k=1}^K  \left( \frac{1}{y^{(k+1)} - y^{(k)}}\right)^{|L^{(k)}|}} \\
\end{align}


Passing to the log:

\begin{align}
log(r) &= \sum_{i<j} \left(  y_{ij} \cdot \log{ p^{\prime}_{z_i z_j} } + (n_{ij} - y_{ij}) \cdot \log{ (1 - p^{\prime}_{z_i z_j}) } \right)  - \sum_{k=1}^K |L^{\prime(k)}| \cdot \log{\left( y^{\prime(k+1)} - y^{\prime(k)} \right)}\\
 &\qquad - \sum_{i<j} \left(  y_{ij} \cdot \log{ p_{z_i z_j} } + (n_{ij} - y_{ij}) \cdot \log{ (1 - p_{z_i z_j}) } \right)  + \sum_{k=1}^K |L^{(k)}| \cdot \log{\left( y^{(k+1)} - y^{(k)} \right) }
\end{align}







\begin{algorithm}
\begin{algorithmic}[1]
\State $j \gets 1$
\While{$j \leq N_{iter}$}
\State Sample $\alpha^{\prime}$ from a truncated normal distribution
\State Generate a new proposal matrix $P^{\prime}$
\State Compute new victory probabilities $p_{z_iz_j}^{\prime}$ using $P^{\prime}$ and $z_{current}$
\State Compute probability ratio $log(r)$ using $p_{z_iz_j}^{\prime}$ and $p_{z_iz_j}$
\State Set $\alpha_{r} \gets \min(1, r)$
\State Sample $u$ from a uniform distribution on $(0,1)$
\If{$u < \alpha_{r}$}
\State Update $\alpha$ to $\alpha^{\prime}$
\State Update $P$ to $P^{\prime}$
\State Update $p_{z_iz_j}$ to $p_{z_iz_j}^{\prime}$
\State Increment $acc.count_{p}$
\EndIf
\State Store $P$ in $P.container$
\State Store $\alpha$ in $\alpha.container$

\State $j \gets j+1$
\EndWhile
\end{algorithmic}
\label{alg:P_update}
\caption{Updating $P$ step}
\end{algorithm}

\section{Appendix II: POMM prior checks}

\subsection{Prior predictive check}

\subsection{MLE check}


\begin{comment}

\section{Idea about a Gibbs sampler}

To derive the full conditional distribution of $\textbf{z}$ given the data $y$ and the hyperparameters $\boldsymbol{\gamma}$, we can use Bayes' theorem and write:

\begin{align*}
p(\textbf{z}|y,\boldsymbol{\gamma}) &\propto p(y|\textbf{z}) p(\textbf{z}|\boldsymbol{\gamma}) \
&\propto \prod_{i=1}^n \prod_{j=i+1}^n {n_{ij} \choose y_{ij}} (p_{z_i,z_j})^{y_{ij}} (1-p_{z_i,z_j})^{n_{ij}-y_{ij}} \prod_{k=1}^K \frac{\Gamma(\gamma_k + m_k)}{\Gamma(\gamma_k)}
\end{align*}

where we have dropped constant terms that do not depend on $\textbf{z}$. We can simplify this expression by collecting terms that depend on each $z_i$. Specifically, we can group the terms in the likelihood that involve $z_i$ with the prior probability of $z_i$ to get:

\begin{align*}
p(z_i|y,\textbf{z}{-i},\boldsymbol{\gamma}) &\propto p(y{i,\cdot}|\textbf{z}) p(z_i|\boldsymbol{\gamma}) \
&= \prod_{j\neq i} {n_{ij} \choose y_{ij}} (p_{z_i,z_j})^{y_{ij}} (1-p_{z_i,z_j})^{n_{ij}-y_{ij}} \frac{\Gamma(\gamma_{z_i} + m_{z_i})}{\Gamma(\gamma_{z_i})}
\end{align*}

where $\textbf{z}{-i}$ denotes all elements of $\textbf{z}$ except for $z_i$, and $y{i,\cdot}$ denotes the $i$th row of the $n\times n$ matrix of observations $y$. We can recognize the above expression as the likelihood of $z_i$ being drawn from a categorical distribution with parameter vector $\boldsymbol{\theta}{-i}$, where $\theta{k,-i} \propto \prod_{j\neq i} (p_{k,z_j})^{y_{ij}} (1-p_{k,z_j})^{n_{ij}-y_{ij}} \frac{\Gamma(\gamma_k + m_k)}{\Gamma(\gamma_k)}$ is the partial likelihood of $z_i$ being assigned value $k$, with $z_j$ for $j\neq i$ fixed to their current values. Thus, we have:

\begin{align*}
p(z_i=k|y,\textbf{z}{-i},\boldsymbol{\gamma}) &= \frac{\theta{k,-i}}{\sum_{k'} \theta_{k',-i}}
\end{align*}

for each possible value of $k$. This gives the full conditional distribution of $z_i$ given the data, the other $z_j$'s, and the hyperparameters.


\section{Possible applications}
The POMM model can have various applications in fields where pairwise comparisons are made. Some examples of applications are:
\begin{itemize}
\item Sports Analytics: The POMM model can be used to rank sports teams based on their pairwise comparison results. It can also be used to predict the probability of a team winning a match against another team.

\item Marketing: The POMM model can be used to rank products based on their pairwise comparison results in surveys. It can also be used to estimate the probability of a customer preferring one product over another.

\item Decision Making: The POMM model can be used to rank options based on their pairwise comparison results. It can also be used to estimate the probability of one option being preferred over another in a decision-making process.

\item Social Science: The POMM model can be used to study social preferences by asking individuals to compare two different options. For example, it can be used to understand people's preferences for different political candidates, policies, or social norms.

\item Biology: The POMM model can be used to study the relative fitness of different genotypes in evolutionary biology or the preferences of animals for different stimuli in behavioral ecology.
\end{itemize}
Overall, the POMM model can be applied in any field where pairwise comparisons are made and where the goal is to rank or estimate the probabilities of different options.



\begin{align}\log \left(\Pr(\mathbf{x}\mid n, \boldsymbol{\alpha})\right) &= \log\left(\frac{\Gamma\left(\alpha_0\right)\Gamma\left(n+1\right)}
{\Gamma\left(n+\alpha_0\right)}\prod_{k=1}^K\frac{\Gamma(x_{k}+\alpha_{k})}{\Gamma(\alpha_{k})\Gamma\left(x_{k}+1\right)}\right) \\
 &= \log\left(\Gamma\left(\alpha_0\right)\right) + \log\left(\Gamma\left(n+1\right)\right) - \log\left(\Gamma\left(n+\alpha_0\right)\right) \\ 
 & \qquad + \sum_{k=1}^K\left[\log\left(\Gamma(x_{k}+\alpha_{k})\right) - \log\left(\Gamma(\alpha_{k})\right) - \log\left(\Gamma\left(x_{k}+1\right)\right)\right] \end{align}

\end{comment}




\end{document}